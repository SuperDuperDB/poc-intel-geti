{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ff80768-7417-4ff3-8e6b-2a29d87d0fa2",
   "metadata": {},
   "source": [
    "# INTEL GETI Docs Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b01f8c2-1826-4932-b2f2-873f1d9403f4",
   "metadata": {},
   "source": [
    "## Using SuperduperDB to Connect to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d36ea08b-07f7-4636-ab93-504911e08329",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhouhaha/workspace/SuperDuperDB/poc-intel-geti/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-03-07 11:56:03,656\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m 2024-Mar-07 11:56:03.66\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mzhouhaha-2.local\u001b[0m| \u001b[36msuperduperdb.base.build\u001b[0m:\u001b[36m65  \u001b[0m | \u001b[1mData Client is ready. MongoClient(host=['mongodb:27017'], document_class=dict, tz_aware=False, connect=True, serverselectiontimeoutms=5000)\u001b[0m\n",
      "\u001b[32m 2024-Mar-07 11:56:03.67\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mzhouhaha-2.local\u001b[0m| \u001b[36msuperduperdb.base.build\u001b[0m:\u001b[36m38  \u001b[0m | \u001b[1mConnecting to Metadata Client with engine:  MongoClient(host=['mongodb:27017'], document_class=dict, tz_aware=False, connect=True, serverselectiontimeoutms=5000)\u001b[0m\n",
      "\u001b[32m 2024-Mar-07 11:56:03.68\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mzhouhaha-2.local\u001b[0m| \u001b[36msuperduperdb.base.build\u001b[0m:\u001b[36m148 \u001b[0m | \u001b[1mConnecting to compute client: local\u001b[0m\n",
      "\u001b[32m 2024-Mar-07 11:56:03.68\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mzhouhaha-2.local\u001b[0m| \u001b[36msuperduperdb.base.datalayer\u001b[0m:\u001b[36m85  \u001b[0m | \u001b[1mBuilding Data Layer\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from superduperdb import superduper\n",
    "import os\n",
    "mongodb_uri = os.getenv(\"SUPERDUPERDB_DATA_BACKEND\",\"mongomock://test\")\n",
    "db = superduper(mongodb_uri)\n",
    "db.drop(force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b1e73c-c22a-44a4-b07a-8a8f8fbcb08c",
   "metadata": {},
   "source": [
    "## Build a rag data processing workflow step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb0f548-d55c-4d08-ac83-be34f0a642cb",
   "metadata": {},
   "source": [
    "### Step1: Crawling Pages\n",
    "\n",
    "**Crawl pages based on the provided links.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83386a19-881a-475a-870b-1de9a7156818",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from urllib.parse import urljoin\n",
    "from superduperdb.misc.retry import Retry\n",
    "from superduperdb import logging\n",
    "\n",
    "\n",
    "def process_code_snippets(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    pre_tags = soup.find_all(\"pre\")\n",
    "\n",
    "    for pre in pre_tags:\n",
    "        processed_text = str(pre.text)\n",
    "        new_content = \"CODE::\" + soup.new_string(processed_text)\n",
    "        pre.clear()\n",
    "        pre.append(new_content)\n",
    "    return str(soup)\n",
    "\n",
    "\n",
    "def process_py_class(source_html):\n",
    "    soup = BeautifulSoup(source_html, \"html.parser\")\n",
    "    dl_tags = soup.find_all(\"dl\", class_=\"py class\")\n",
    "\n",
    "    for dl in dl_tags:\n",
    "        dt_tag = dl.find(\"dt\", class_=\"sig sig-object py\")\n",
    "        if not dt_tag:\n",
    "            continue\n",
    "        last_headerlink = dt_tag.find_all(\"a\", class_=\"headerlink\")[-1]\n",
    "        href = last_headerlink[\"href\"] if last_headerlink else \"\"\n",
    "        id = dt_tag.attrs[\"id\"]\n",
    "        new_h3 = soup.new_tag(\"h3\")\n",
    "        new_a_inside_h3 = soup.new_tag(\"a\", href=href)\n",
    "        new_a_inside_h3.string = f\"Class: {id}\"\n",
    "        new_h3.append(new_a_inside_h3)\n",
    "\n",
    "        new_code = soup.new_tag(\"a\")\n",
    "        new_code.string = dt_tag.text\n",
    "        dt_tag.insert_before(new_h3)\n",
    "        dt_tag.insert_before(new_code)\n",
    "        dt_tag.decompose()\n",
    "\n",
    "    return str(soup)\n",
    "\n",
    "def parse_url(seed_url):\n",
    "    retry = Retry(exception_types=(Exception))\n",
    "\n",
    "    @retry\n",
    "    def get_response(url):\n",
    "        response = requests.get(seed_url)\n",
    "        return response\n",
    "        \n",
    "    print(f\"parse {seed_url}\")\n",
    "    response = get_response(seed_url)\n",
    "    # Parse the HTML content\n",
    "    source_html = response.text\n",
    "    source_html = process_code_snippets(source_html)\n",
    "    source_html = process_py_class(source_html)\n",
    "\n",
    "    return source_html\n",
    "\n",
    "\n",
    "def url2html(url):\n",
    "    try:\n",
    "        html = parse_url(url)\n",
    "    except Exception as e:\n",
    "        logging.error(e)\n",
    "        html = \"\"\n",
    "    return html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b831d645-f41c-4b0b-9a96-166e6e62e77e",
   "metadata": {},
   "source": [
    "**Now we can test the `url2html` function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23d2f420-e59e-49db-9fe8-098ecbd67df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parse https://openvinotoolkit.github.io/geti-sdk/getting_started.html\n"
     ]
    }
   ],
   "source": [
    "page = url2html(\"https://openvinotoolkit.github.io/geti-sdk/getting_started.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02e6acd-d0ce-47e1-9ec4-4a976c84729c",
   "metadata": {},
   "source": [
    "**After we confirm that this function is working properly, we can add it as a model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0abc62d7-9a9d-46fa-b89b-61ac08a55baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m 2024-Mar-07 11:56:04.36\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mzhouhaha-2.local\u001b[0m| \u001b[36msuperduperdb.components.component\u001b[0m:\u001b[36m333 \u001b[0m | \u001b[1mInitializing DataType : dill\u001b[0m\n",
      "\u001b[32m 2024-Mar-07 11:56:04.36\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mzhouhaha-2.local\u001b[0m| \u001b[36msuperduperdb.components.component\u001b[0m:\u001b[36m336 \u001b[0m | \u001b[1mInitialized  DataType : dill successfully\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([],\n",
       " ObjectModel(identifier='url2html', signature='*args,**kwargs', datatype=None, output_schema=None, flatten=False, model_update_kwargs={'document_embedded': False}, metrics=(), validation_sets=None, predict_kwargs={}, object=<function url2html at 0x162cf9990>, num_workers=0))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from superduperdb import Model, Listener, Schema\n",
    "from superduperdb.backends.mongodb import Collection\n",
    "\n",
    "url_model = Model(\n",
    "    identifier='url2html',\n",
    "    object=url2html,\n",
    "    model_update_kwargs={\"document_embedded\": False},\n",
    ")\n",
    "db.add(url_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80bd02a-83d3-4489-8abe-c1dde0a3d344",
   "metadata": {},
   "source": [
    "### Step2: Parse html and chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f9b902-094f-425b-bf30-738448e7883c",
   "metadata": {},
   "source": [
    "**Use unstructured to extract elements of html page**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a5b22b3-0440-4e54-8409-22b1777362fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.partition.html import partition_html\n",
    "\n",
    "def page2elements(page):\n",
    "    elements = partition_html(text=page, html_assemble_articles=True)\n",
    "    return elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f4f327a-7da9-4e19-b3fb-2241c7f810db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-03-07 11:56:05] unstructured INFO Reading document from string ...\n",
      "[2024-03-07 11:56:05] unstructured INFO Reading document ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction\n",
      "\n",
      "Welcome to the Intel® Geti™ SDK! The Intel® Geti™ platform enables\n",
      "teams to rapidly develop AI models. The platform reduces the time needed to build\n",
      "models by easing the complexities of model development and harnessing greater\n",
      "collaboration between teams. Most importantly, the platform unlocks faster\n",
      "time-to-value for digitization initiatives with AI.\n",
      "\n",
      "The Intel® Geti™ SDK is a python package which contains tools to interact with an\n",
      "Intel® Geti™ server via the REST API. It provides functionality for:\n",
      "\n",
      "Project creation from annotated datasets on disk\n",
      "\n",
      "Project downloading (images, videos, configuration, annotations, predictions and models)\n"
     ]
    }
   ],
   "source": [
    "elements = page2elements(page)\n",
    "print('\\n\\n'.join([e.text for e in elements[:5]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9797f9-440b-41ef-9e36-07cf7e43b59e",
   "metadata": {},
   "source": [
    "## In this application, we use titles to segment text, so we first define a function for title recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c27170ff-bad8-468d-a309-0879b7a9ae84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.documents.elements import ElementType\n",
    "\n",
    "def get_title_data(element):\n",
    "    data = {}\n",
    "    if element.category != ElementType.TITLE:\n",
    "        return data\n",
    "    if 'link_urls' not in element.metadata.to_dict():\n",
    "        return data\n",
    "\n",
    "    if 'category_depth' not in element.metadata.to_dict():\n",
    "        return data\n",
    "\n",
    "    [link_text, *_] = element.metadata.link_texts\n",
    "\n",
    "    if not link_text:\n",
    "        return data\n",
    "\n",
    "    link_urls = element.metadata.link_urls\n",
    "    if not link_urls:\n",
    "        return data\n",
    "    category_depth = element.metadata.category_depth\n",
    "    return {'link': link_urls[0], 'category_depth':category_depth}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56698869-af06-4d62-82d3-042cf47c5b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'link': '#introduction', 'category_depth': 0}\n"
     ]
    }
   ],
   "source": [
    "print(get_title_data(elements[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea71ab4-9ab8-440e-9c42-69e1d58e1829",
   "metadata": {},
   "source": [
    "**Define a function that converts element to text, and handles different types of elements differently.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f808d13-cdf8-4315-8cf6-b7eb09cd7583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "def element2text(element):\n",
    "    title_message = get_title_data(element)\n",
    "    text = element.text\n",
    "    if title_message:\n",
    "        title_tags = '#' * (title_message['category_depth'] + 1)\n",
    "        text = title_tags + ' ' + text\n",
    "        text = text.rstrip('#')\n",
    "\n",
    "    elif element.category == ElementType.LIST_ITEM:\n",
    "        text = '- ' + text\n",
    "\n",
    "    elif element.category == ElementType.TABLE:\n",
    "        html = element.metadata.text_as_html\n",
    "        html = html.replace('|', '')\n",
    "        df = pd.read_html(StringIO(html))[0]\n",
    "        text = df.to_markdown(index=False)\n",
    "        text = text + '  \\n'\n",
    "\n",
    "    if text.startswith(\"CODE::\"):\n",
    "        text = f\"```\\n{text[6:]}\\n```\"\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "359d2ff9-65a3-4daa-ae83-3904e897db34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Intel® Geti™ SDK! The Intel® Geti™ platform enables\n",
      "teams to rapidly develop AI models. The platform reduces the time needed to build\n",
      "models by easing the complexities of model development and harnessing greater\n",
      "collaboration between teams. Most importantly, the platform unlocks faster\n",
      "time-to-value for digitization initiatives with AI.\n"
     ]
    }
   ],
   "source": [
    "print(element2text(elements[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db38519-11bb-4918-8c35-95360a223a1e",
   "metadata": {},
   "source": [
    "**Define the chunk function, input all elements of a page, and chunk them**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c5f92ec-8cc0-4eb6-a85f-16445f3c025e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunk_texts(text, chunk_size=1000, overlap_size=300):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "\n",
    "    while start < len(text):\n",
    "        if chunks:\n",
    "            start -= overlap_size\n",
    "        end = start + chunk_size\n",
    "        end = min(end, len(text))\n",
    "        chunks.append(text[start:end])\n",
    "        start = end\n",
    "        if start >= len(text):\n",
    "            break\n",
    "\n",
    "    return chunks\n",
    "\n",
    "from collections import defaultdict\n",
    "def get_chunks(elements):\n",
    "    chunk_tree = defaultdict(list)\n",
    "    now_depth = -1\n",
    "    now_path = 'root'\n",
    "    for element in elements:\n",
    "        title_data = get_title_data(element)\n",
    "        if not title_data:\n",
    "            chunk_tree[now_path].append(element)\n",
    "        else:\n",
    "            link = title_data['link']\n",
    "            depth = title_data['category_depth']\n",
    "            if depth > now_depth:\n",
    "                now_path = now_path + \"::\" +link\n",
    "            else:\n",
    "                now_path = '::'.join(now_path.split(\"::\")[:depth+1] + [link])\n",
    "            now_depth = depth\n",
    "            chunk_tree[now_path].append(element)\n",
    "     \n",
    "    chunks = []\n",
    "    for node_path, node_elements in chunk_tree.items():\n",
    "        new_elements = []\n",
    "        nodes = node_path.split(\"::\")\n",
    "        parent_elements = []\n",
    "        for i in range(1, len(nodes) - 1):\n",
    "            [parent_element, *_] = chunk_tree[\"::\".join(nodes[:i+1])] or [None]\n",
    "            if parent_element:\n",
    "                parent_elements.append(parent_element)\n",
    "        node_elements = [*parent_elements, *node_elements]\n",
    "        content = '\\n\\n'.join(map(lambda x: element2text(x), node_elements))\n",
    "        for chunk_text in get_chunk_texts(content):\n",
    "            # The url field is used to save the jump link\n",
    "            # The text field is used for vector search\n",
    "            # The content field is used to submit to LLM for answer\n",
    "            chunk = {\"href\": nodes[-1], 'text': chunk_text, 'content': content}\n",
    "            chunks.append(chunk)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26d0e1fc-4bcd-485b-9084-cc7ff719e1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = get_chunks(elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "210744d2-6a0e-472f-8103-b583a91abdfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'href': '#introduction', 'text': '# Introduction\\uf0c1\\n\\nWelcome to the Intel® Geti™ SDK! The Intel® Geti™ platform enables\\nteams to rapidly develop AI models. The platform reduces the time needed to build\\nmodels by easing the complexities of model development and harnessing greater\\ncollaboration between teams. Most importantly, the platform unlocks faster\\ntime-to-value for digitization initiatives with AI.\\n\\nThe Intel® Geti™ SDK is a python package which contains tools to interact with an\\nIntel® Geti™ server via the REST API. It provides functionality for:\\n\\n- Project creation from annotated datasets on disk\\n\\n- Project downloading (images, videos, configuration, annotations, predictions and models)\\n\\n- Project creation and upload from a previous download\\n\\n- Deploying a project for local inference with OpenVINO\\n\\n- Getting and setting project and model configuration\\n\\n- Launching and monitoring training jobs\\n\\n- Media upload and prediction\\n\\nThis repository also contains a set of (tutorial style) Jupyter\\nnotebooks\\nthat demonstrate ', 'content': '# Introduction\\uf0c1\\n\\nWelcome to the Intel® Geti™ SDK! The Intel® Geti™ platform enables\\nteams to rapidly develop AI models. The platform reduces the time needed to build\\nmodels by easing the complexities of model development and harnessing greater\\ncollaboration between teams. Most importantly, the platform unlocks faster\\ntime-to-value for digitization initiatives with AI.\\n\\nThe Intel® Geti™ SDK is a python package which contains tools to interact with an\\nIntel® Geti™ server via the REST API. It provides functionality for:\\n\\n- Project creation from annotated datasets on disk\\n\\n- Project downloading (images, videos, configuration, annotations, predictions and models)\\n\\n- Project creation and upload from a previous download\\n\\n- Deploying a project for local inference with OpenVINO\\n\\n- Getting and setting project and model configuration\\n\\n- Launching and monitoring training jobs\\n\\n- Media upload and prediction\\n\\nThis repository also contains a set of (tutorial style) Jupyter\\nnotebooks\\nthat demonstrate how to use the SDK. We highly recommend checking them out to get a\\nfeeling for use cases for the package.'}\n",
      "{'href': '#introduction', 'text': 'om a previous download\\n\\n- Deploying a project for local inference with OpenVINO\\n\\n- Getting and setting project and model configuration\\n\\n- Launching and monitoring training jobs\\n\\n- Media upload and prediction\\n\\nThis repository also contains a set of (tutorial style) Jupyter\\nnotebooks\\nthat demonstrate how to use the SDK. We highly recommend checking them out to get a\\nfeeling for use cases for the package.', 'content': '# Introduction\\uf0c1\\n\\nWelcome to the Intel® Geti™ SDK! The Intel® Geti™ platform enables\\nteams to rapidly develop AI models. The platform reduces the time needed to build\\nmodels by easing the complexities of model development and harnessing greater\\ncollaboration between teams. Most importantly, the platform unlocks faster\\ntime-to-value for digitization initiatives with AI.\\n\\nThe Intel® Geti™ SDK is a python package which contains tools to interact with an\\nIntel® Geti™ server via the REST API. It provides functionality for:\\n\\n- Project creation from annotated datasets on disk\\n\\n- Project downloading (images, videos, configuration, annotations, predictions and models)\\n\\n- Project creation and upload from a previous download\\n\\n- Deploying a project for local inference with OpenVINO\\n\\n- Getting and setting project and model configuration\\n\\n- Launching and monitoring training jobs\\n\\n- Media upload and prediction\\n\\nThis repository also contains a set of (tutorial style) Jupyter\\nnotebooks\\nthat demonstrate how to use the SDK. We highly recommend checking them out to get a\\nfeeling for use cases for the package.'}\n",
      "{'href': '#getting-started', 'text': '# Getting started\\uf0c1', 'content': '# Getting started\\uf0c1'}\n"
     ]
    }
   ],
   "source": [
    "for chunk in chunks[:3]:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aac6029-250d-4977-be45-256764a15021",
   "metadata": {},
   "source": [
    "**Now we finally define a function that converts html pages into chunks, so that we can connect it to the page output by the model we defined above.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a75dc722-7cd6-4243-ad2c-e9090b8d2f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def page2chunks(page):\n",
    "    elements = page2elements(page)\n",
    "    chunks = get_chunks(elements)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d798696-8c30-424f-bf6d-e335856fb146",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-03-07 11:56:06] unstructured INFO Reading document from string ...\n",
      "[2024-03-07 11:56:06] unstructured INFO Reading document ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'href': '#introduction',\n",
       " 'text': '# Introduction\\uf0c1\\n\\nWelcome to the Intel® Geti™ SDK! The Intel® Geti™ platform enables\\nteams to rapidly develop AI models. The platform reduces the time needed to build\\nmodels by easing the complexities of model development and harnessing greater\\ncollaboration between teams. Most importantly, the platform unlocks faster\\ntime-to-value for digitization initiatives with AI.\\n\\nThe Intel® Geti™ SDK is a python package which contains tools to interact with an\\nIntel® Geti™ server via the REST API. It provides functionality for:\\n\\n- Project creation from annotated datasets on disk\\n\\n- Project downloading (images, videos, configuration, annotations, predictions and models)\\n\\n- Project creation and upload from a previous download\\n\\n- Deploying a project for local inference with OpenVINO\\n\\n- Getting and setting project and model configuration\\n\\n- Launching and monitoring training jobs\\n\\n- Media upload and prediction\\n\\nThis repository also contains a set of (tutorial style) Jupyter\\nnotebooks\\nthat demonstrate ',\n",
       " 'content': '# Introduction\\uf0c1\\n\\nWelcome to the Intel® Geti™ SDK! The Intel® Geti™ platform enables\\nteams to rapidly develop AI models. The platform reduces the time needed to build\\nmodels by easing the complexities of model development and harnessing greater\\ncollaboration between teams. Most importantly, the platform unlocks faster\\ntime-to-value for digitization initiatives with AI.\\n\\nThe Intel® Geti™ SDK is a python package which contains tools to interact with an\\nIntel® Geti™ server via the REST API. It provides functionality for:\\n\\n- Project creation from annotated datasets on disk\\n\\n- Project downloading (images, videos, configuration, annotations, predictions and models)\\n\\n- Project creation and upload from a previous download\\n\\n- Deploying a project for local inference with OpenVINO\\n\\n- Getting and setting project and model configuration\\n\\n- Launching and monitoring training jobs\\n\\n- Media upload and prediction\\n\\nThis repository also contains a set of (tutorial style) Jupyter\\nnotebooks\\nthat demonstrate how to use the SDK. We highly recommend checking them out to get a\\nfeeling for use cases for the package.'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = page2chunks(page)\n",
    "chunks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b635ce76-40d6-4dd5-856f-2a36afefc3ec",
   "metadata": {},
   "source": [
    "**After we confirm that this function is working properly, we can add it as a model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23d14067-7e6c-4a0c-bc0d-b3f866576b2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([],\n",
       " ObjectModel(identifier='chunk', signature='*args,**kwargs', datatype=None, output_schema=None, flatten=True, model_update_kwargs={'document_embedded': False}, metrics=(), validation_sets=None, predict_kwargs={}, object=<function page2chunks at 0x29614c820>, num_workers=0))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from superduperdb import Model, Listener, Schema\n",
    "\n",
    "chunk_model = Model(\n",
    "    identifier='chunk',\n",
    "    object=page2chunks,\n",
    "    flatten=True,\n",
    "    model_update_kwargs={\"document_embedded\": False},\n",
    ")\n",
    "db.add(chunk_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81a4f85-00a8-4534-827e-8a7ab25be070",
   "metadata": {},
   "source": [
    "### Step3: Embedding\n",
    "\n",
    "**We will embedding all chunk**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a578ca19-b5d2-4696-abe8-9e4058387c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduperdb.ext.openai import OpenAIEmbedding\n",
    "from superduperdb import VectorIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61d55e30-1e18-4698-8fdb-052f6a775c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-03-07 11:56:07] httpx INFO HTTP Request: GET https://api.openai.com/v1/models \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([],\n",
       " OpenAIEmbedding(identifier='text-embedding-ada-002', datatype=DataType(identifier='vector[1536]', encoder=None, decoder=None, info=None, shape=(1536,), directory=None, encodable='native', bytes_encoding=<BytesEncoding.BYTES: 'Bytes'>), output_schema=None, flatten=False, model_update_kwargs={}, metrics=(), validation_sets=None, predict_kwargs={}, model='text-embedding-ada-002', client_kwargs={}, shape=(1536,), batch_size=100))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai_emb_model = OpenAIEmbedding(\n",
    "    identifier='text-embedding-ada-002',\n",
    "    model=\"text-embedding-ada-002\",\n",
    ")\n",
    "db.add(openai_emb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0010679c-c9f9-4294-ae8f-a47129b53566",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-03-07 11:56:08] httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536\n"
     ]
    }
   ],
   "source": [
    "print(len(openai_emb_model.predict_one(chunk[\"content\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d69e9e2-b0ba-452e-ad6b-a5c5f64f42ec",
   "metadata": {},
   "source": [
    "**In order to be compatible with the database’s dict format and the application’s string format data, we add a preprocessing model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d71401f-bfa7-4808-9f77-ba56f819bfd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m 2024-Mar-07 11:56:08.19\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mzhouhaha-2.local\u001b[0m| \u001b[36msuperduperdb.components.component\u001b[0m:\u001b[36m333 \u001b[0m | \u001b[1mInitializing ObjectModel : get_content\u001b[0m\n",
      "\u001b[32m 2024-Mar-07 11:56:08.19\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mzhouhaha-2.local\u001b[0m| \u001b[36msuperduperdb.components.component\u001b[0m:\u001b[36m336 \u001b[0m | \u001b[1mInitialized  ObjectModel : get_content successfully\u001b[0m\n",
      "# Getting started\n"
     ]
    }
   ],
   "source": [
    "content_model = Model(\n",
    "    identifier=\"get_content\",\n",
    "    object=lambda x:x['text'] if isinstance(x, dict) else x,\n",
    ")\n",
    "\n",
    "print(content_model.predict_one(chunk))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12984bb-c935-41b3-aba4-c2493eebe273",
   "metadata": {},
   "source": [
    "**We can use A to easily connect multiple models in series**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e57ebd1f-155d-4899-b89b-c035bb45e88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduperdb.components.model import SequentialModel\n",
    "embed_model = SequentialModel(identifier=\"embedding\", predictors=[content_model, openai_emb_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f60f7e59-3791-46db-abc0-f6233ae0aaac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                        | 0/1 [00:00<?, ?it/s][2024-03-07 11:56:08] httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(len(embed_model.predict_one(chunk)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c768d8-689e-4a45-8598-3f4a60752492",
   "metadata": {},
   "source": [
    "### Step4: LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7bdb25c6-5799-4a8b-870d-9000ae659e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chunk', 'gpt-3.5-turbo', 'text-embedding-ada-002', 'url2html']\n"
     ]
    }
   ],
   "source": [
    "from superduperdb.ext.openai import OpenAIChatCompletion\n",
    "prompt = \"\"\"\n",
    "As an Intel GETI assistant, based on the provided documents and the question, answer the question.\n",
    "If the document does not provide an answer, offer a safe response without fabricating an answer.\n",
    "\n",
    "Documents:\n",
    "{context}\n",
    "\n",
    "Question: \"\"\"\n",
    "\n",
    "llm = OpenAIChatCompletion(identifier='gpt-3.5-turbo', prompt=prompt)\n",
    "\n",
    "db.add(llm)\n",
    "\n",
    "print(db.show('model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a6dcb494-1e1c-4adf-ac14-6106781a06f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Introduction\\uf0c1\\n\\nWelcome to the Intel® Geti™ SDK! The Intel® Geti™ platform enables\\nteams to rapidly develop AI models. The platform reduces the time needed to build\\nmodels by easing the complexities of model development and harnessing greater\\ncollaboration between teams. Most importantly, the platform unlocks faster\\ntime-to-value for digitization initiatives with AI.\\n\\nThe Intel® Geti™ SDK is a python package which contains tools to interact with an\\nIntel® Geti™ server via the REST API. It provides functionality for:\\n\\n- Project creation from annotated datasets on disk\\n\\n- Project downloading (images, videos, configuration, annotations, predictions and models)\\n\\n- Project creation and upload from a previous download\\n\\n- Deploying a project for local inference with OpenVINO\\n\\n- Getting and setting project and model configuration\\n\\n- Launching and monitoring training jobs\\n\\n- Media upload and prediction\\n\\nThis repository also contains a set of (tutorial style) Jupyter\\nnotebooks\\nthat demonstrate '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = chunks[0]['text']\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "780b8d37-20e2-44c7-ba7a-57e0a5f3ac36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-03-07 11:56:12] httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The Intel Geti™ SDK is a platform that enables teams to rapidly develop AI models. It reduces the time needed to build models by simplifying the complexities of model development and promoting greater collaboration between teams. Moreover, the platform unlocks faster time-to-value for digitization initiatives with AI. The SDK is a Python package that includes tools to interact with an Intel Geti™ server via the REST API. It provides functionality for project creation from annotated datasets on disk, project downloading (images, videos, configuration, annotations, predictions, and models), project creation and upload from a previous download, deploying a project for local inference with OpenVINO, getting and setting project and model configuration, launching and monitoring training jobs, and media upload and prediction. Additionally, the repository contains a set of tutorial-style Jupyter notebooks that demonstrate the capabilities of the Geti™ SDK.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.predict_one(\"Introduce Geti™ SDK!\", context=context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2fa8cb-7c0f-4a6c-a859-d909b5e1d87a",
   "metadata": {},
   "source": [
    "## Concatenate the above data workflow and add it to the CDC service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d60922-4b7d-417c-9d3a-35e84d644e73",
   "metadata": {},
   "source": [
    "### Step1: Crawling Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1cc61d80-b3cc-4e84-b421-b235a7634a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m 2024-Mar-07 11:56:12.53\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mzhouhaha-2.local\u001b[0m| \u001b[36msuperduperdb.backends.local.compute\u001b[0m:\u001b[36m32  \u001b[0m | \u001b[1mSubmitting job. function:<function method_job at 0x1246dda20>\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m 2024-Mar-07 11:56:12.54\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mzhouhaha-2.local\u001b[0m| \u001b[36msuperduperdb.components.component\u001b[0m:\u001b[36m333 \u001b[0m | \u001b[1mInitializing ObjectModel : url2html\u001b[0m\n",
      "\u001b[32m 2024-Mar-07 11:56:12.55\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mzhouhaha-2.local\u001b[0m| \u001b[36msuperduperdb.components.component\u001b[0m:\u001b[36m333 \u001b[0m | \u001b[1mInitializing DataType : dill\u001b[0m\n",
      "\u001b[32m 2024-Mar-07 11:56:12.55\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mzhouhaha-2.local\u001b[0m| \u001b[36msuperduperdb.components.component\u001b[0m:\u001b[36m336 \u001b[0m | \u001b[1mInitialized  DataType : dill successfully\u001b[0m\n",
      "\u001b[32m 2024-Mar-07 11:56:12.55\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mzhouhaha-2.local\u001b[0m| \u001b[36msuperduperdb.components.component\u001b[0m:\u001b[36m336 \u001b[0m | \u001b[1mInitialized  ObjectModel : url2html successfully\u001b[0m\n",
      "\u001b[32m 2024-Mar-07 11:56:12.55\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mzhouhaha-2.local\u001b[0m| \u001b[36msuperduperdb.components.model\u001b[0m:\u001b[36m649 \u001b[0m | \u001b[1mAdding 0 model outputs to `db`\u001b[0m\n",
      "\u001b[32m 2024-Mar-07 11:56:12.55\u001b[0m| \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mzhouhaha-2.local\u001b[0m| \u001b[36msuperduperdb.backends.local.compute\u001b[0m:\u001b[36m38  \u001b[0m | \u001b[32m\u001b[1mJob submitted.  function:<function method_job at 0x1246dda20> future:1d798627-7399-4c23-85d4-fb22f335492e\u001b[0m\n",
      "url2html/url _outputs.url.url2html.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "url_listener = Listener(\n",
    "    model=url_model,\n",
    "    select=Collection(\"url\").find(),\n",
    "    key=\"url\",\n",
    ")\n",
    "db.add(url_listener)\n",
    "print(url_listener.identifier, url_listener.outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3068d8ee-d290-483a-b85f-2a95b1b9888c",
   "metadata": {},
   "source": [
    "### Step2: Parse html and chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b9f40cf4-2bfe-4c7b-a603-0075ea200e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m 2024-Mar-07 11:56:12.70\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mzhouhaha-2.local\u001b[0m| \u001b[36msuperduperdb.backends.local.compute\u001b[0m:\u001b[36m32  \u001b[0m | \u001b[1mSubmitting job. function:<function method_job at 0x1246dda20>\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m 2024-Mar-07 11:56:12.70\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mzhouhaha-2.local\u001b[0m| \u001b[36msuperduperdb.components.component\u001b[0m:\u001b[36m333 \u001b[0m | \u001b[1mInitializing ObjectModel : chunk\u001b[0m\n",
      "\u001b[32m 2024-Mar-07 11:56:12.70\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mzhouhaha-2.local\u001b[0m| \u001b[36msuperduperdb.components.component\u001b[0m:\u001b[36m336 \u001b[0m | \u001b[1mInitialized  ObjectModel : chunk successfully\u001b[0m\n",
      "\u001b[32m 2024-Mar-07 11:56:12.70\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mzhouhaha-2.local\u001b[0m| \u001b[36msuperduperdb.components.model\u001b[0m:\u001b[36m649 \u001b[0m | \u001b[1mAdding 0 model outputs to `db`\u001b[0m\n",
      "\u001b[32m 2024-Mar-07 11:56:12.71\u001b[0m| \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mzhouhaha-2.local\u001b[0m| \u001b[36msuperduperdb.backends.local.compute\u001b[0m:\u001b[36m38  \u001b[0m | \u001b[32m\u001b[1mJob submitted.  function:<function method_job at 0x1246dda20> future:bafb1b8e-dbfd-4205-b713-beeb80605c3e\u001b[0m\n",
      "chunk/_outputs.url.url2html.0 _outputs._outputs.url.url2html.0.chunk.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "chunk_listener = Listener(\n",
    "    model=chunk_model,\n",
    "    select=Collection(\"_outputs.url.url2html\").find(),\n",
    "    key=f'_outputs.url.url2html.{url_listener.model.version}',\n",
    ")\n",
    "\n",
    "db.add(chunk_listener)\n",
    "\n",
    "print(chunk_listener.identifier, chunk_listener.outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7e7b7e-9caa-4c47-9f01-cf095ada12a9",
   "metadata": {},
   "source": [
    "### Step3: Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c92d69ec-476e-4555-a9db-78735e6e3d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding/_outputs.url.chunk.0 _outputs._outputs.url.chunk.0.embedding.None\n",
      "\u001b[32m 2024-Mar-07 11:56:13.73\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mzhouhaha-2.local\u001b[0m| \u001b[36msuperduperdb.backends.local.compute\u001b[0m:\u001b[36m32  \u001b[0m | \u001b[1mSubmitting job. function:<function method_job at 0x1246dda20>\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m 2024-Mar-07 11:56:13.76\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mzhouhaha-2.local\u001b[0m| \u001b[36msuperduperdb.components.component\u001b[0m:\u001b[36m333 \u001b[0m | \u001b[1mInitializing ObjectModel : get_content\u001b[0m\n",
      "\u001b[32m 2024-Mar-07 11:56:13.77\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mzhouhaha-2.local\u001b[0m| \u001b[36msuperduperdb.components.component\u001b[0m:\u001b[36m336 \u001b[0m | \u001b[1mInitialized  ObjectModel : get_content successfully\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m 2024-Mar-07 11:56:13.77\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mzhouhaha-2.local\u001b[0m| \u001b[36msuperduperdb.components.model\u001b[0m:\u001b[36m649 \u001b[0m | \u001b[1mAdding 0 model outputs to `db`\u001b[0m\n",
      "\u001b[32m 2024-Mar-07 11:56:13.77\u001b[0m| \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mzhouhaha-2.local\u001b[0m| \u001b[36msuperduperdb.backends.local.compute\u001b[0m:\u001b[36m38  \u001b[0m | \u001b[32m\u001b[1mJob submitted.  function:<function method_job at 0x1246dda20> future:88e17098-2978-4d13-8d91-f827e0f80142\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([<superduperdb.jobs.job.ComponentJob at 0x1629eea10>],\n",
       " Listener(identifier='embedding/_outputs.url.chunk.0', key='_outputs.url.chunk.0', model=SequentialModel(identifier='embedding', signature='*args,**kwargs', datatype=DataType(identifier='vector[1536]', encoder=None, decoder=None, info=None, shape=(1536,), directory=None, encodable='native', bytes_encoding=<BytesEncoding.BYTES: 'Bytes'>), output_schema=None, flatten=False, model_update_kwargs={}, metrics=(), validation_sets=None, predict_kwargs={}, predictors=[ObjectModel(identifier='get_content', signature='*args,**kwargs', datatype=None, output_schema=None, flatten=False, model_update_kwargs={}, metrics=(), validation_sets=None, predict_kwargs={}, object=<function <lambda> at 0x29614dea0>, num_workers=0), OpenAIEmbedding(identifier='text-embedding-ada-002', datatype=DataType(identifier='vector[1536]', encoder=None, decoder=None, info=None, shape=(1536,), directory=None, encodable='native', bytes_encoding=<BytesEncoding.BYTES: 'Bytes'>), output_schema=None, flatten=False, model_update_kwargs={}, metrics=(), validation_sets=None, predict_kwargs={}, model='text-embedding-ada-002', client_kwargs={}, shape=(1536,), batch_size=100)]), select=<superduperdb.backends.mongodb.query.MongoCompoundSelect[\n",
       "     \u001b[92m\u001b[1m_outputs.url.chunk.url.chunk.find({}, {})\u001b[0m\n",
       " ] object at 0x2973c8c40>, active=True, predict_kwargs={'max_chunk_size': 64}))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_listener = Listener(\n",
    "    select=Collection(\"_outputs.url.chunk\").find(),\n",
    "    key=f'_outputs.url.chunk.{chunk_listener.model.version}',  # Key for the documents\n",
    "    model=embed_model,  # Specify the model for processing\n",
    "    predict_kwargs={\"max_chunk_size\": 64},\n",
    ")\n",
    "print(embed_listener.identifier, embed_listener.outputs)\n",
    "db.add(embed_listener)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56a90d2-1be1-4585-959f-f2cb9d5ebcb2",
   "metadata": {},
   "source": [
    "## Create a vector index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "59dcef08-90c0-4b61-bed9-dc1a36b1a8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m 2024-Mar-07 11:56:13.81\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mzhouhaha-2.local\u001b[0m| \u001b[36msuperduperdb.backends.local.compute\u001b[0m:\u001b[36m32  \u001b[0m | \u001b[1mSubmitting job. function:<function callable_job at 0x1246ddcf0>\u001b[0m\n",
      "\u001b[32m 2024-Mar-07 11:56:13.82\u001b[0m| \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mzhouhaha-2.local\u001b[0m| \u001b[36msuperduperdb.backends.local.compute\u001b[0m:\u001b[36m38  \u001b[0m | \u001b[32m\u001b[1mJob submitted.  function:<function callable_job at 0x1246ddcf0> future:76925104-f71f-46dc-8ff3-7d514ebfbced\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([<superduperdb.jobs.job.FunctionJob at 0x2973c93f0>],\n",
       " VectorIndex(identifier='vector_index', indexing_listener=Listener(identifier='embedding/_outputs.url.chunk.0', key='_outputs.url.chunk.0', model=SequentialModel(identifier='embedding', signature='*args,**kwargs', datatype=DataType(identifier='vector[1536]', encoder=None, decoder=None, info=None, shape=(1536,), directory=None, encodable='native', bytes_encoding=<BytesEncoding.BYTES: 'Bytes'>), output_schema=None, flatten=False, model_update_kwargs={}, metrics=(), validation_sets=None, predict_kwargs={}, predictors=[ObjectModel(identifier='get_content', signature='*args,**kwargs', datatype=None, output_schema=None, flatten=False, model_update_kwargs={}, metrics=(), validation_sets=None, predict_kwargs={}, object=<function <lambda> at 0x29614dea0>, num_workers=0), OpenAIEmbedding(identifier='text-embedding-ada-002', datatype=DataType(identifier='vector[1536]', encoder=None, decoder=None, info=None, shape=(1536,), directory=None, encodable='native', bytes_encoding=<BytesEncoding.BYTES: 'Bytes'>), output_schema=None, flatten=False, model_update_kwargs={}, metrics=(), validation_sets=None, predict_kwargs={}, model='text-embedding-ada-002', client_kwargs={}, shape=(1536,), batch_size=100)]), select=<superduperdb.backends.mongodb.query.MongoCompoundSelect[\n",
       "     \u001b[92m\u001b[1m_outputs.url.chunk.url.chunk.find({}, {})\u001b[0m\n",
       " ] object at 0x2973c8c40>, active=True, predict_kwargs={'max_chunk_size': 64}), compatible_listener=None, measure=<VectorIndexMeasureType.cosine: 'cosine'>, metric_values={}))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_index = VectorIndex(\n",
    "    identifier=\"vector_index\",\n",
    "    indexing_listener=embed_listener,)\n",
    "db.add(vector_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0461c441-6006-4424-b02c-e08eb5be3f55",
   "metadata": {},
   "source": [
    "## Create a Rag application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccf3613-f87b-4d4d-b40b-bec1266f9c6c",
   "metadata": {},
   "source": [
    "**Insert a web page**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0c0b1542-6881-404d-8c98-a5c6aa200afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m 2024-Mar-07 11:56:13.87\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mzhouhaha-2.local\u001b[0m| \u001b[36msuperduperdb.base.datalayer\u001b[0m:\u001b[36m384 \u001b[0m | \u001b[1mCDC active, skipping refresh\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([ObjectId('65e93add75c2fdf140a0a91a')], None)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from superduperdb import Document\n",
    "url = \"https://openvinotoolkit.github.io/geti-sdk/getting_started.html\"\n",
    "db.execute(Collection(\"url\").insert_one(Document(**{\"url\": url})), refresh=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c579eb55-50b1-4d3c-a87a-5ac704cdb84b",
   "metadata": {},
   "source": [
    "**Wait a moment**\n",
    "- the CDC service will run the data pipeline\n",
    "- the vector search service will update the new vector index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b73d113d-6ee0-44d9-836c-771dcca5effb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211ddec8-f274-4b20-b56e-6341d942487f",
   "metadata": {},
   "source": [
    "### Vector Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e1f82e76-53dd-4db7-9ca2-c361926f320f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_search(query):\n",
    "    outs = db.execute(\n",
    "        Collection(\"_outputs.url.chunk\")\n",
    "        .like(Document({\"_outputs.url.chunk.0\": query}), vector_index=\"vector_index\", n=3)\n",
    "        .find()\n",
    "    )\n",
    "    if outs:\n",
    "        outs = sorted(outs, key=lambda x: x[\"score\"], reverse=True)\n",
    "    for out in outs:\n",
    "        print(\"-\" * 20, \"\\n\")\n",
    "        data = out.outputs(\"url\", \"chunk\")\n",
    "        url = data[\"href\"]\n",
    "        print(url, out[\"score\"])\n",
    "        print(data[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5dcf3093-bb98-459e-a51c-48122ef4a817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m 2024-Mar-07 11:56:18.98\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mzhouhaha-2.local\u001b[0m| \u001b[36msuperduperdb.base.datalayer\u001b[0m:\u001b[36m1047\u001b[0m | \u001b[1m{}\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                        | 0/1 [00:00<?, ?it/s][2024-03-07 11:56:19] httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- \n",
      "\n",
      "#installation 0.8307849168777466\n",
      "# Getting started\n",
      "\n",
      "## Installation\n",
      "\n",
      "Using an environment manager such as\n",
      "Anaconda or\n",
      "venv to create a new\n",
      "Python environment before installing the Intel® Geti™ SDK and its requirements is\n",
      "highly recommended.\n",
      "\n",
      "NOTE: If you have installed multiple versions of Python,\n",
      "use py -3.8 venv -m <env_name> when creating your virtual environment to specify\n",
      "a supported version (in this case 3.8). Once you activate the\n",
      "virtual environment <venv_path>/Scripts/activate, make sure to upgrade pip\n",
      "to the latest version python -m pip install --upgrade pip wheel setuptools.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vector_search(\"How to install python sdk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8131d8a-ef2c-430d-9017-1db48a6c0e7d",
   "metadata": {},
   "source": [
    "### QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c71f484c-2cf8-4fdf-9060-3f685366e2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa(query, vector_search_top_k=5):\n",
    "    collection = Collection(\"_outputs.url.chunk\")\n",
    "    output, sources = db.predict(\n",
    "        model_name=\"gpt-3.5-turbo\",\n",
    "        input=query,\n",
    "        context_select=collection.like(\n",
    "            Document({\"_outputs.url.chunk.0\": query}),\n",
    "            vector_index=\"vector_index\",\n",
    "            n=vector_search_top_k,\n",
    "        ).find({}),\n",
    "        context_key=\"_outputs.url.chunk.0.text\",\n",
    "    )\n",
    "    if sources:\n",
    "        sources = sorted(sources, key=lambda x: x[\"score\"], reverse=True)\n",
    "    print(output.unpack())\n",
    "    for out in sources:\n",
    "        print(\"-\" * 20, \"\\n\")\n",
    "        data = out.outputs(\"url\", \"chunk\")\n",
    "        url = data[\"href\"]\n",
    "        print(url, out[\"score\"])\n",
    "        print(data[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1bde6b0c-2437-499d-9287-00652fe03a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m 2024-Mar-07 11:56:20.17\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mzhouhaha-2.local\u001b[0m| \u001b[36msuperduperdb.base.datalayer\u001b[0m:\u001b[36m1047\u001b[0m | \u001b[1m{}\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                        | 0/1 [00:00<?, ?it/s][2024-03-07 11:56:20] httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.44it/s]\n",
      "[2024-03-07 11:56:23] httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To install the Python SDK, it is recommended to use an environment manager such as Anaconda or venv to create a new Python environment. Then, you can follow the steps mentioned in the \"Installation\" section of the provided documents, depending on your needs:\n",
      "\n",
      "- For base installation: Navigate to the root directory of the repository and install the SDK using `pip install .`.\n",
      "- For notebooks installation (optional): If you want to run notebooks, install extra requirements using `pip install .[notebooks]`.\n",
      "- For development installation (optional): To run tests or build documentation, install the package extra requirements by using `pip install -e .[dev]`. \n",
      "\n",
      "Follow these steps to install the Python SDK.\n",
      "-------------------- \n",
      "\n",
      "#installation 0.8307616710662842\n",
      "# Getting started\n",
      "\n",
      "## Installation\n",
      "\n",
      "Using an environment manager such as\n",
      "Anaconda or\n",
      "venv to create a new\n",
      "Python environment before installing the Intel® Geti™ SDK and its requirements is\n",
      "highly recommended.\n",
      "\n",
      "NOTE: If you have installed multiple versions of Python,\n",
      "use py -3.8 venv -m <env_name> when creating your virtual environment to specify\n",
      "a supported version (in this case 3.8). Once you activate the\n",
      "virtual environment <venv_path>/Scripts/activate, make sure to upgrade pip\n",
      "to the latest version python -m pip install --upgrade pip wheel setuptools.\n",
      "-------------------- \n",
      "\n",
      "#installing-from-the-git-repo 0.8263670802116394\n",
      "# Getting started\n",
      "\n",
      "## Installation\n",
      "\n",
      "### Installing from the Git repo\n",
      "\n",
      "- Download or clone the repository and navigate to the root directory of the repo in\n",
      "your terminal.\n",
      "\n",
      "- Base installation Within this directory, install the SDK using pip install . This command will install the\n",
      "package and its base dependencies in your environment.\n",
      "\n",
      "- Notebooks installation (Optional) If you want to be able to run the notebooks, make sure to\n",
      "install the extra requirements using pip install .[notebooks] This will install both the\n",
      "SDK and all other dependencies needed to run the notebooks in your environment\n",
      "\n",
      "- Development installation (Optional) If you plan on running the tests or want to build the\n",
      "documentation, you can install the package extra requirements by doing for example\n",
      "pip install -e .[dev]\n",
      "The valid options for the extra requirements are [dev, docs, notebooks],\n",
      "corresponding to the following functionality:\n",
      "\n",
      "dev Install requirements to run the test suite on your local machine\n",
      "notebooks In\n"
     ]
    }
   ],
   "source": [
    "qa(\"How to install python sdk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c4d046-845c-4aa6-8d00-78770d013c22",
   "metadata": {},
   "source": [
    "**Now we crawl all web page URL collections of geti and add them to the database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b6248232-de0a-4d67-858b-fa1ac32c76bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "retry = Retry(exception_types=(Exception))\n",
    "\n",
    "def is_toctree_class(tag):\n",
    "    classes = tag.get('class', [])\n",
    "    return any(re.match('toctree-l\\d+', cls) for cls in classes)\n",
    "\n",
    "def filter_sub_urls(all_urls):\n",
    "    # remove the URL with #, for example: http://xxxx.com/xxx#P1\n",
    "    base_urls_set = {url for url in all_urls if '#' not in url}\n",
    "    new_urls = []\n",
    "    for url in all_urls:\n",
    "        if '#' in url and url.split('#')[0] in base_urls_set:\n",
    "            continue\n",
    "        else:\n",
    "            new_urls.append(url)\n",
    "    return new_urls\n",
    "\n",
    "@retry\n",
    "def get_documentation_links(seed_url):\n",
    "    response = requests.get(seed_url)\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    page_urls = []\n",
    "    for l in soup.find_all(is_toctree_class):\n",
    "        page_name = l.find('a').text.strip()\n",
    "        href = l.find('a')['href'] if l.find('a') else ''\n",
    "        if href:\n",
    "            url = urljoin(seed_url, href)\n",
    "            page_urls.append(url)\n",
    "\n",
    "    page_urls = filter_sub_urls(page_urls)\n",
    "            \n",
    "    return page_urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9816902a-eb54-4277-8180-e97e23e296eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://openvinotoolkit.github.io/geti-sdk/getting_started.html',\n",
       " 'https://openvinotoolkit.github.io/geti-sdk/notebooks.html',\n",
       " 'https://openvinotoolkit.github.io/geti-sdk/contributing_to_the_sdk.html',\n",
       " 'https://openvinotoolkit.github.io/geti-sdk/api_reference.html']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_documentation_links(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "39b3e67b-677e-4a37-998d-9fa0e5d07545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number to be check 1.  https://openvinotoolkit.github.io/geti-sdk/index.html\n",
      "The number to be check 4.  https://openvinotoolkit.github.io/geti-sdk/api_reference.html\n",
      "The number to be check 13.  https://openvinotoolkit.github.io/geti-sdk/geti_sdk.rest_clients.html\n",
      "The number to be check 12.  https://docs.geti.intel.com/on-prem/1.8/guide/get-started/introduction.html\n",
      "The number to be check 68.  https://openvinotoolkit.github.io/geti-sdk/getting_started.html\n",
      "The number to be check 67.  https://docs.geti.intel.com/on-prem/1.8/guide/release-notes/1.0-beta/release-1.0-beta.html\n",
      "The number to be check 66.  https://docs.geti.intel.com/on-prem/1.8/guide/additional-resources/openvino/test-optimize-deploy-openvino.html\n",
      "The number to be check 65.  https://docs.geti.intel.com/on-prem/1.8/guide/installation-guide/installation.html\n",
      "The number to be check 64.  https://docs.geti.intel.com/on-prem/1.8/guide/additional-resources/ai-fundamentals/detection-project.html\n",
      "The number to be check 63.  https://openvinotoolkit.github.io/geti-sdk/notebooks.html\n",
      "The number to be check 62.  https://docs.geti.intel.com/on-prem/1.8/guide/installation-guide/upgrade/upgradeNew-guide.html\n",
      "The number to be check 61.  https://docs.geti.intel.com/on-prem/1.8/guide/installation-guide/installation/preparing-server-environment.html\n",
      "The number to be check 60.  https://docs.geti.intel.com/on-prem/1.8/guide/release-notes/1.4/release-1.4.html\n",
      "The number to be check 59.  https://docs.geti.intel.com/on-prem/1.8/guide/labels/labels-management.html\n",
      "The number to be check 58.  https://docs.geti.intel.com/on-prem/1.8/guide/datasets/statistics.html\n",
      "The number to be check 57.  https://docs.geti.intel.com/on-prem/1.8/guide/installation-guide/additional-resources/troubleshooting.html\n",
      "The number to be check 56.  https://docs.geti.intel.com/on-prem/1.8/guide/release-notes/1.5.1/release-1.5.1.html\n",
      "The number to be check 55.  https://docs.geti.intel.com/on-prem/1.8/guide/installation-guide/upgrade.html\n",
      "The number to be check 54.  https://docs.geti.intel.com/on-prem/1.8/guide/installation-guide/additional-resources/backup-and-recovery.html\n",
      "The number to be check 53.  https://docs.geti.intel.com/on-prem/1.8/guide/release-notes/1.5.3/release-1.5.3.html\n",
      "The number to be check 52.  https://docs.geti.intel.com/on-prem/1.8/guide/datasets/media.html\n",
      "The number to be check 51.  https://docs.geti.intel.com/on-prem/1.8/guide/release-notes/1.1/release-1.1.html\n",
      "The number to be check 50.  https://docs.geti.intel.com/on-prem/1.8/guide/get-started/supported-tasks.html\n",
      "The number to be check 49.  https://docs.geti.intel.com/on-prem/1.8/guide/annotations/annotation-editor.html\n",
      "The number to be check 48.  https://docs.geti.intel.com/on-prem/1.8/guide/get-started/tutorials/tutorials.html\n",
      "The number to be check 47.  https://docs.geti.intel.com/on-prem/1.8/guide/account-management/account-management.html\n",
      "The number to be check 46.  https://docs.geti.intel.com/on-prem/1.8/guide/release-notes/1.2/release-1.2.html\n",
      "The number to be check 45.  https://openvinotoolkit.github.io/geti-sdk/geti_sdk.deployment.html\n",
      "The number to be check 44.  https://docs.geti.intel.com/on-prem/1.8/guide/installation-guide/installation/downloading-package.html\n",
      "The number to be check 43.  https://docs.geti.intel.com/on-prem/1.8/guide/rest-api/rest-api-redirect.html\n",
      "The number to be check 42.  https://docs.geti.intel.com/on-prem/1.8/guide/additional-resources/responsible-ai.html\n",
      "The number to be check 41.  https://docs.geti.intel.com/on-prem/1.8/guide/release-notes/1.5/release-1.5.html\n",
      "The number to be check 40.  https://docs.geti.intel.com/on-prem/1.8/guide/get-started/downloadable-dataset.html\n",
      "The number to be check 39.  https://docs.geti.intel.com/on-prem/1.8/guide/annotations/annotation-tools.html\n",
      "The number to be check 38.  https://docs.geti.intel.com/on-prem/1.8/guide/additional-resources/ai-fundamentals/dataset-creation.html\n",
      "The number to be check 37.  https://docs.geti.intel.com/on-prem/1.8/guide/release-notes/1.8/release-1.8.html\n",
      "The number to be check 36.  https://docs.geti.intel.com/on-prem/1.8/guide/installation-guide/additional-resources/uninstall-guide.html\n",
      "The number to be check 35.  https://openvinotoolkit.github.io/geti-sdk/geti_sdk.rest_converters.html\n",
      "The number to be check 34.  https://openvinotoolkit.github.io/geti-sdk/geti_sdk.data_models.html\n",
      "The number to be check 33.  https://openvinotoolkit.github.io/geti-sdk/geti_sdk.demos.html\n",
      "The number to be check 32.  https://openvinotoolkit.github.io/geti-sdk/geti_sdk.deployment.data_models.html\n",
      "The number to be check 31.  https://docs.geti.intel.com/on-prem/1.8/guide/tests-management/tests.html\n",
      "The number to be check 30.  https://openvinotoolkit.github.io/geti-sdk/contributing_to_the_sdk.html\n",
      "The number to be check 31.  https://docs.geti.intel.com/on-prem/1.8/guide/release-notes/release-notes.html\n",
      "The number to be check 30.  https://docs.geti.intel.com/on-prem/1.8/guide/installation-guide/installation/license-activation.html\n",
      "The number to be check 29.  https://docs.geti.intel.com/on-prem/1.8/guide/installation-guide/installation/hw-sw-requirements.html\n",
      "The number to be check 28.  https://docs.geti.intel.com/on-prem/1.8/guide/installation-guide/additional-resources/security-considerations.html\n",
      "The number to be check 27.  https://openvinotoolkit.github.io/geti-sdk/geti.html\n",
      "The number to be check 26.  https://openvinotoolkit.github.io/geti-sdk/geti_sdk.annotation_readers.html\n",
      "The number to be check 25.  https://docs.geti.intel.com/on-prem/1.8/guide/additional-resources/ai-fundamentals/ai-fundamentals-tasks.html\n",
      "The number to be check 24.  https://docs.geti.intel.com/on-prem/1.8/guide/datasets/dataset-management.html\n",
      "The number to be check 23.  https://docs.geti.intel.com/on-prem/1.8/guide/additional-resources/ai-fundamentals/ai-fundamentals-index.html\n",
      "The number to be check 22.  https://docs.geti.intel.com/on-prem/1.8/guide/additional-resources/ai-fundamentals/classification-project.html\n",
      "The number to be check 21.  https://docs.geti.intel.com/on-prem/1.8/guide/additional-resources/ai-fundamentals/active-learning.html\n",
      "The number to be check 20.  https://docs.geti.intel.com/on-prem/1.8/guide/annotations/video-annotation.html\n",
      "The number to be check 19.  https://docs.geti.intel.com/on-prem/1.8/guide/installation-guide/installation/install-guide.html\n",
      "The number to be check 18.  https://docs.geti.intel.com/on-prem/1.8/guide/annotations/annotation-mode.html\n",
      "The number to be check 17.  https://docs.geti.intel.com/on-prem/1.8/guide/installation-guide/additional-resources.html\n",
      "The number to be check 16.  https://docs.geti.intel.com/on-prem/1.8/guide/release-notes/1.5.2/release-1.5.2.html\n",
      "The number to be check 15.  https://openvinotoolkit.github.io/geti-sdk/contributing.html\n",
      "The number to be check 14.  https://docs.geti.intel.com/on-prem/1.8/guide/additional-resources/ai-fundamentals/segmentation-project.html\n",
      "The number to be check 13.  https://docs.geti.intel.com/on-prem/1.8/guide/additional-resources/ai-fundamentals/anomaly-classification-project.html\n",
      "The number to be check 12.  https://openvinotoolkit.github.io/geti-sdk/geti_sdk.utils.html\n",
      "The number to be check 11.  https://docs.geti.intel.com/on-prem/1.8/guide/release-notes/1.3/release-1.3.html\n",
      "The number to be check 10.  https://docs.geti.intel.com/on-prem/1.8/guide/model-training-and-optimization/model-training-and-optimization.html\n",
      "The number to be check 9.  https://docs.geti.intel.com/on-prem/1.8/guide/additional-resources/custom-model.html\n",
      "The number to be check 8.  https://openvinotoolkit.github.io/geti-sdk/geti_sdk.http_session.html\n",
      "The number to be check 7.  https://docs.geti.intel.com/on-prem/1.8/guide/installation-guide/upgrade/downloading-package.html\n",
      "The number to be check 6.  https://docs.geti.intel.com/on-prem/1.8/guide/installation-guide/upgrade/before-upgrade.html\n",
      "The number to be check 5.  https://docs.geti.intel.com/on-prem/1.8/guide/rest-api/rest-api.html\n",
      "The number to be check 4.  https://docs.geti.intel.com/on-prem/1.8/guide/project-management/project-management.html\n",
      "The number to be check 3.  https://docs.geti.intel.com/on-prem/1.8/guide/installation-guide/additional-resources/best-practices.html\n",
      "The number to be check 2.  https://docs.geti.intel.com/on-prem/1.8/guide/additional-resources/ai-fundamentals/task-chaining-project.html\n",
      "The number to be check 1.  https://docs.geti.intel.com/on-prem/1.8/guide/deployments/deployments.html\n",
      "The number to be check 0.  https://openvinotoolkit.github.io/geti-sdk/testing.html\n"
     ]
    }
   ],
   "source": [
    "# URL of the page to scrape\n",
    "url_sets = set()\n",
    "url_sets.add(\"https://openvinotoolkit.github.io/geti-sdk/index.html\")\n",
    "url_sets.add(\"https://docs.geti.intel.com/on-prem/1.8/guide/get-started/introduction.html\")\n",
    "url_waiting_list = url_sets.copy()\n",
    "while url_waiting_list:\n",
    "    url = url_waiting_list.pop()\n",
    "    print(f'The number to be check {len(url_waiting_list)}. ', url)\n",
    "    new_urls = get_documentation_links(url)\n",
    "    new_urls ={url for url in new_urls if url not in url_sets}\n",
    "    url_waiting_list.update(new_urls)\n",
    "    url_sets.update(new_urls)\n",
    "    \n",
    "# Delete this data because we added it in the beginning\n",
    "url_sets.remove(\"https://openvinotoolkit.github.io/geti-sdk/index.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4ffb1c2c-23ac-4190-a683-ef201429adda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m 2024-Mar-07 11:57:32.26\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mzhouhaha-2.local\u001b[0m| \u001b[36msuperduperdb.base.datalayer\u001b[0m:\u001b[36m1047\u001b[0m | \u001b[1m{}\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                        | 0/1 [00:00<?, ?it/s][2024-03-07 11:57:32] httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.48it/s]\n",
      "[2024-03-07 11:57:35] httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In version 1.8, the supported features include model upload, prediction upload, and exporting datasets to COCO/YOLO/VOC format. Additionally, the export functionality from the Intel® Geti™ user interface can be used for exporting datasets. The features not yet supported in version 1.8 but will be added in future releases are fetching the active dataset, triggering model optimization, running model tests, and creating datasets and retrieving dataset statistics.\n",
      "-------------------- \n",
      "\n",
      "#supported-features 0.7988542914390564\n",
      "# Supported features\n",
      "-------------------- \n",
      "\n",
      "#what-is-not-supported 0.7762789130210876\n",
      "# Supported features\n",
      "\n",
      "## What is not supported\n",
      "\n",
      "- Model upload\n",
      "\n",
      "- Prediction upload\n",
      "\n",
      "- Exporting datasets to COCO/YOLO/VOC format: For this, you can use the export\n",
      "functionality from the Intel® Geti™ user interface instead.\n",
      "\n",
      "The following features are not supported yet but will be added to the SDK in future\n",
      "releases:\n",
      "\n",
      "- Fetching the active dataset\n",
      "\n",
      "- Triggering (post-training) model optimization\n",
      "\n",
      "- Running model tests\n",
      "\n",
      "- Creating datasets and retrieving dataset statistics\n"
     ]
    }
   ],
   "source": [
    "qa(\"What features are released in version 1.8?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f58212b8-b22e-4a2e-9291-34787322feca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m 2024-Mar-07 11:57:35.23\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mzhouhaha-2.local\u001b[0m| \u001b[36msuperduperdb.base.datalayer\u001b[0m:\u001b[36m384 \u001b[0m | \u001b[1mCDC active, skipping refresh\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([ObjectId('65e93b2f75c2fdf140a0a91b'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a91c'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a91d'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a91e'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a91f'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a920'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a921'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a922'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a923'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a924'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a925'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a926'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a927'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a928'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a929'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a92a'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a92b'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a92c'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a92d'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a92e'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a92f'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a930'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a931'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a932'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a933'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a934'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a935'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a936'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a937'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a938'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a939'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a93a'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a93b'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a93c'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a93d'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a93e'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a93f'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a940'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a941'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a942'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a943'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a944'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a945'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a946'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a947'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a948'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a949'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a94a'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a94b'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a94c'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a94d'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a94e'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a94f'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a950'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a951'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a952'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a953'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a954'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a955'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a956'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a957'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a958'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a959'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a95a'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a95b'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a95c'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a95d'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a95e'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a95f'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a960'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a961'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a962'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a963'),\n",
       "  ObjectId('65e93b2f75c2fdf140a0a964')],\n",
       " None)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datas = [Document(**{\"url\": url}) for url in url_sets]\n",
    "db.execute(Collection(\"url\").insert_many(datas), refresh=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cba1d54-f537-4a82-8855-5e8e57a609df",
   "metadata": {},
   "source": [
    "**We need to sleep longer because the CDC service needs to run for a long time, and crawling dozens of web pages is time-consuming.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8a15cfbf-b3df-4081-8f88-a9dbab7b5de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a97bb668-d220-45ac-b173-271e3952e7cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m 2024-Mar-07 11:59:35.39\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mzhouhaha-2.local\u001b[0m| \u001b[36msuperduperdb.base.datalayer\u001b[0m:\u001b[36m1047\u001b[0m | \u001b[1m{}\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                        | 0/1 [00:00<?, ?it/s][2024-03-07 11:59:36] httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.26it/s]\n",
      "[2024-03-07 11:59:39] httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided documents, the features released in Intel® Geti™ 1.8.0 include:\n",
      "\n",
      "- Enhanced labeling experience with the Automatic Segmentation tool\n",
      "- Sample datasets available\n",
      "- New storage tab\n",
      "- Project size display\n",
      "- Video player improvements\n",
      "- Removal of Filter Pruning\n",
      "- Download individual media\n",
      "- Active model architecture indication\n",
      "\n",
      "These updates and feature enhancements are part of the Intel® Geti™ 1.8.0 release.\n",
      "-------------------- \n",
      "\n",
      "#release-details 0.8630382418632507\n",
      "# IntelÂ® Getiâ¢ 1.8.0\n",
      "\n",
      "## Release Details\n",
      "\n",
      "This section covers additional details on the new functionality available with IntelÂ® Getiâ¢ 1.8.0.\n",
      "-------------------- \n",
      "\n",
      "#intel-geti-1-8-0 0.8435623049736023\n",
      "# IntelÂ® Getiâ¢ 1.8.0\n",
      "-------------------- \n",
      "\n",
      "#release-summary 0.826368510723114\n",
      "# IntelÂ® Getiâ¢ 1.8.0\n",
      "\n",
      "## Release Summary\n",
      "\n",
      "IntelÂ® Getiâ¢ 1.8.0 contains several updates and feature enhancements, including key highlights:\n",
      "\n",
      "- Enhanced labeling experience with the Automatic Segmentation tool — Segment objects and create bounding boxes with a click.\n",
      "\n",
      "- Sample datasets available - Get started by building models with ready-to-use sample data.\n",
      "\n",
      "- New storage tab - Access storage usage and per-project usage information.\n",
      "\n",
      "Other IntelÂ® Getiâ¢ 1.8.0 updates include:\n",
      "\n",
      "- Project size display — Track the size of your projects directly in the Projects tab.\n",
      "\n",
      "- Video player improvements - Navigate through video data smoothly with an enhanced media player.\n",
      "\n",
      "- Removal of Filter Pruning - Filter Pruning as a model optimization capability has been removed.\n",
      "\n",
      "- Download individual media - Extract specific image or video files available in the datasets directly from the UI.\n",
      "\n",
      "- Active model architecture indication - Keep track of the active model architecture and version details on t\n",
      "-------------------- \n",
      "\n",
      "#video-player-improvements 0.8213381171226501\n",
      "# IntelÂ® Getiâ¢ 1.8.0\n",
      "\n",
      "## Release Details\n",
      "\n",
      "### Video player improvements\n",
      "\n",
      "The IntelÂ® Getiâ¢ platform 1.8.0 brings along a suite of enhancements aimed at improving the functionality and user experience of the video player.\n",
      "\n",
      "Key improvements include:\n",
      "\n",
      "- The video player timeline will accurately show predictions and annotations during the video playback, sometimes requiring a buffer time for a seamless viewing experience.\n",
      "\n",
      "- During playback of video files, if the playback speed is adjusted, the video will seamlessly continue playing at the new speed.\n",
      "\n",
      "- In case a video playback reaches the end, clicking the play' button will restart the video from the beginning.\n",
      "\n",
      "- After the completion of a video, the frame marker will now align with the last frame.\n",
      "-------------------- \n",
      "\n",
      "#project-size-display 0.8204203844070435\n",
      "# IntelÂ® Getiâ¢ 1.8.0\n",
      "\n",
      "## Release Details\n",
      "\n",
      "### Project size display\n",
      "\n",
      "Project size information is now available in the Projects tab by selecting the details view.\n"
     ]
    }
   ],
   "source": [
    "qa(\"What features are released in version 1.8?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e960e10e-4b56-4c2e-a63f-8c72810b78a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
