{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89fc820f-7b6c-4add-8b2d-e70069a6d0db",
   "metadata": {},
   "source": [
    "# INTEL GETI Docs Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3923a0-d10f-44b7-9c8a-8cc499953173",
   "metadata": {},
   "source": [
    "Install related dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8580979-8994-41a1-818a-24bef3cec47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install superduperdb unstructured pandas openai aiohttp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3885e6a8-6118-4e0f-9875-f35470421479",
   "metadata": {},
   "source": [
    "## Crawling Pages\n",
    "\n",
    "Crawl pages based on the provided links. Additionally, retrieve a list of new pages from the sidebar directory information and continue crawling until all pages have been crawled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6964c15-4c01-4d20-a6d4-5d6469983f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parse https://docs.geti.intel.com/on-prem/1.8/guide/get-started/introduction.html\n",
      "parse https://docs.geti.intel.com/on-prem/1.8/guide/account-management/account-management.html\n",
      "parse https://docs.geti.intel.com/on-prem/1.8/guide/additional-resources/openvino/test-optimize-deploy-openvino.html\n",
      "parse https://docs.geti.intel.com/on-prem/1.8/guide/datasets/statistics.html\n",
      "parse https://docs.geti.intel.com/on-prem/1.8/guide/additional-resources/ai-fundamentals/anomaly-classification-project.html\n",
      "parse https://docs.geti.intel.com/on-prem/1.8/guide/rest-api/rest-api-redirect.html\n",
      "parse https://docs.geti.intel.com/on-prem/1.8/guide/annotations/video-annotation.html\n",
      "parse https://docs.geti.intel.com/on-prem/1.8/guide/installation-guide/additional-resources/uninstall-guide.html\n",
      "parse https://docs.geti.intel.com/on-prem/1.8/guide/installation-guide/installation.html\n",
      "parse https://docs.geti.intel.com/on-prem/1.8/guide/datasets/media.html\n",
      "parse https://docs.geti.intel.com/on-prem/1.8/guide/get-started/supported-tasks.html\n",
      "parse https://docs.geti.intel.com/on-prem/1.8/guide/annotations/annotation-mode.html\n",
      "parse https://docs.geti.intel.com/on-prem/1.8/guide/installation-guide/additional-resources/security-considerations.html\n",
      "parse https://docs.geti.intel.com/on-prem/1.8/guide/rest-api/rest-api.html\n",
      "parse https://docs.geti.intel.com/on-prem/1.8/guide/installation-guide/upgrade/before-upgrade.html\n",
      "parse https://docs.geti.intel.com/on-prem/1.8/guide/release-notes/1.0-beta/release-1.0-beta.html\n",
      "parse https://docs.geti.intel.com/on-prem/1.8/guide/release-notes/release-notes.html\n",
      "parse https://docs.geti.intel.com/on-prem/1.8/guide/project-management/project-management.html\n",
      "parse https://docs.geti.intel.com/on-prem/1.8/guide/installation-guide/installation/downloading-package.html\n",
      "parse https://docs.geti.intel.com/on-prem/1.8/guide/release-notes/1.8/release-1.8.html\n",
      "parse https://docs.geti.intel.com/on-prem/1.8/guide/installation-guide/upgrade/downloading-package.html\n",
      "parse https://docs.geti.intel.com/on-prem/1.8/guide/get-started/tutorials/tutorials.html\n",
      "parse https://docs.geti.intel.com/on-prem/1.8/guide/installation-guide/upgrade/upgradeNew-guide.html\n",
      "parse https://docs.geti.intel.com/on-prem/1.8/guide/installation-guide/additional-resources/backup-and-recovery.html\n",
      "parse https://docs.geti.intel.com/on-prem/1.8/guide/installation-guide/additional-resources/troubleshooting.html\n",
      "parse https://docs.geti.intel.com/on-prem/1.8/guide/labels/labels-management.html\n",
      "parse https://docs.geti.intel.com/on-prem/1.8/guide/installation-guide/installation/preparing-server-environment.html\n",
      "parse https://docs.geti.intel.com/on-prem/1.8/guide/deployments/deployments.html\n",
      "parse https://docs.geti.intel.com/on-prem/1.8/guide/additional-resources/ai-fundamentals/ai-fundamentals-index.html\n",
      "parse https://docs.geti.intel.com/on-prem/1.8/guide/additional-resources/ai-fundamentals/detection-project.html\n",
      "parse https://docs.geti.intel.com/on-prem/1.8/guide/installation-guide/installation/hw-sw-requirements.html\n",
      "parse https://docs.geti.intel.com/on-prem/1.8/guide/additional-resources/ai-fundamentals/task-chaining-project.html\n",
      "parse https://docs.geti.intel.com/on-prem/1.8/guide/installation-guide/additional-resources/best-practices.html\n",
      "parse https://docs.geti.intel.com/on-prem/1.8/guide/release-notes/1.4/release-1.4.html\n",
      "parse https://docs.geti.intel.com/on-prem/1.8/guide/release-notes/1.5.1/release-1.5.1.html\n",
      "parse https://docs.geti.intel.com/on-prem/1.8/guide/release-notes/1.1/release-1.1.html\n",
      "parse https://docs.geti.intel.com/on-prem/1.8/guide/installation-guide/installation/install-guide.html\n",
      "parse https://docs.geti.intel.com/on-prem/1.8/guide/get-started/downloadable-dataset.html\n",
      "parse https://docs.geti.intel.com/on-prem/1.8/guide/tests-management/tests.html\n",
      "parse https://docs.geti.intel.com/on-prem/1.8/guide/annotations/annotation-editor.html\n",
      "parse https://openvinotoolkit.github.io/geti-sdk/index.html\n",
      "parse https://docs.geti.intel.com/on-prem/1.8/guide/installation-guide/upgrade.html\n",
      "parse https://docs.geti.intel.com/on-prem/1.8/guide/release-notes/1.3/release-1.3.html\n",
      "parse https://docs.geti.intel.com/on-prem/1.8/guide/release-notes/1.5.2/release-1.5.2.html\n",
      "parse https://docs.geti.intel.com/on-prem/1.8/guide/additional-resources/ai-fundamentals/ai-fundamentals-tasks.html\n",
      "parse https://docs.geti.intel.com/on-prem/1.8/guide/additional-resources/custom-model.html\n",
      "parse https://docs.geti.intel.com/on-prem/1.8/guide/datasets/dataset-management.html\n",
      "parse https://docs.geti.intel.com/on-prem/1.8/guide/release-notes/1.2/release-1.2.html\n",
      "parse https://docs.geti.intel.com/on-prem/1.8/guide/additional-resources/ai-fundamentals/classification-project.html\n",
      "parse https://docs.geti.intel.com/on-prem/1.8/guide/additional-resources/ai-fundamentals/segmentation-project.html\n",
      "parse https://docs.geti.intel.com/on-prem/1.8/guide/additional-resources/ai-fundamentals/active-learning.html\n",
      "parse https://docs.geti.intel.com/on-prem/1.8/guide/additional-resources/responsible-ai.html\n",
      "parse https://docs.geti.intel.com/on-prem/1.8/guide/model-training-and-optimization/model-training-and-optimization.html\n",
      "parse https://docs.geti.intel.com/on-prem/1.8/guide/installation-guide/additional-resources.html\n",
      "parse https://docs.geti.intel.com/on-prem/1.8/guide/installation-guide/installation/license-activation.html\n",
      "parse https://docs.geti.intel.com/on-prem/1.8/guide/annotations/annotation-tools.html\n",
      "parse https://docs.geti.intel.com/on-prem/1.8/guide/release-notes/1.5.3/release-1.5.3.html\n",
      "parse https://docs.geti.intel.com/on-prem/1.8/guide/additional-resources/ai-fundamentals/dataset-creation.html\n",
      "parse https://docs.geti.intel.com/on-prem/1.8/guide/release-notes/1.5/release-1.5.html\n",
      "parse https://openvinotoolkit.github.io/geti-sdk/getting_started.html\n",
      "parse https://openvinotoolkit.github.io/geti-sdk/api_reference.html\n",
      "parse https://openvinotoolkit.github.io/geti-sdk/geti_sdk.utils.html\n",
      "parse https://openvinotoolkit.github.io/geti-sdk/contributing_to_the_sdk.html\n",
      "parse https://openvinotoolkit.github.io/geti-sdk/geti_sdk.data_models.html\n",
      "parse https://openvinotoolkit.github.io/geti-sdk/contributing.html\n",
      "parse https://openvinotoolkit.github.io/geti-sdk/notebooks.html\n",
      "parse https://openvinotoolkit.github.io/geti-sdk/geti_sdk.http_session.html\n",
      "parse https://openvinotoolkit.github.io/geti-sdk/geti_sdk.rest_converters.html\n",
      "parse https://openvinotoolkit.github.io/geti-sdk/geti_sdk.deployment.html\n",
      "parse https://openvinotoolkit.github.io/geti-sdk/geti_sdk.demos.html\n",
      "parse https://openvinotoolkit.github.io/geti-sdk/testing.html\n",
      "parse https://openvinotoolkit.github.io/geti-sdk/geti_sdk.deployment.data_models.html\n",
      "parse https://openvinotoolkit.github.io/geti-sdk/geti_sdk.rest_clients.html\n",
      "parse https://openvinotoolkit.github.io/geti-sdk/geti_sdk.annotation_readers.html\n",
      "parse https://openvinotoolkit.github.io/geti-sdk/geti.html\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "def is_toctree_class(tag):\n",
    "    classes = tag.get('class', [])\n",
    "    return any(re.match('toctree-l\\d+', cls) for cls in classes)\n",
    "\n",
    "def filter_sub_urls(all_urls):\n",
    "    # remove the URL with #, for example: http://xxxx.com/xxx#P1\n",
    "    base_urls_set = {url for _, url in all_urls if '#' not in url}\n",
    "    new_urls = []\n",
    "    for page_name, url in all_urls:\n",
    "        if '#' in url and url.split('#')[0] in base_urls_set:\n",
    "            continue\n",
    "        else:\n",
    "            new_urls.append((page_name, url))\n",
    "    return new_urls\n",
    "\n",
    "def process_code_snippets(text):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    pre_tags = soup.find_all('pre')\n",
    "\n",
    "    for pre in pre_tags:\n",
    "        processed_text = str(pre.text)\n",
    "        new_content = \"CODE::\"+soup.new_string(processed_text)\n",
    "        pre.clear()\n",
    "        pre.append(new_content)\n",
    "    return str(soup)\n",
    "            \n",
    "    \n",
    "def process_py_class(source_html):\n",
    "    soup = BeautifulSoup(source_html, 'html.parser')\n",
    "    dl_tags = soup.find_all('dl', class_='py class')\n",
    "    \n",
    "    for dl in dl_tags:\n",
    "        dt_tag = dl.find('dt', class_='sig sig-object py')\n",
    "        if not dt_tag:\n",
    "            continue\n",
    "        last_headerlink = dt_tag.find_all('a', class_='headerlink')[-1]\n",
    "        href = last_headerlink['href'] if last_headerlink else ''\n",
    "        id = dt_tag.attrs['id']\n",
    "        new_h3 = soup.new_tag(\"h3\")\n",
    "        new_a_inside_h3 = soup.new_tag(\"a\", href=href)\n",
    "        new_a_inside_h3.string = f\"Class: {id}\"\n",
    "        new_h3.append(new_a_inside_h3)\n",
    "        \n",
    "        new_code = soup.new_tag(\"a\")\n",
    "        new_code.string = dt_tag.text\n",
    "        dt_tag.insert_before(new_h3)\n",
    "        dt_tag.insert_before(new_code)\n",
    "        dt_tag.decompose()\n",
    "        \n",
    "            \n",
    "    return str(soup)\n",
    "\n",
    "def parse_url(seed_url):\n",
    "    print(f\"parse {seed_url}\")\n",
    "    response = requests.get(seed_url)\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    page_urls = []\n",
    "    for l in soup.find_all(is_toctree_class):\n",
    "        page_name = l.find('a').text.strip()\n",
    "        href = l.find('a')['href'] if l.find('a') else ''\n",
    "        if href:\n",
    "            url = urljoin(seed_url, href)\n",
    "            page_urls.append((page_name, url))\n",
    "\n",
    "    page_urls = filter_sub_urls(page_urls)\n",
    "    source_html = response.text\n",
    "    source_html = process_code_snippets(source_html)\n",
    "    source_html = process_py_class(source_html)\n",
    "            \n",
    "    return source_html, page_urls\n",
    "\n",
    "# URL of the page to scrape\n",
    "url_sets = set()\n",
    "url_sets.add(\"https://openvinotoolkit.github.io/geti-sdk/index.html\")\n",
    "url_sets.add(\"https://docs.geti.intel.com/on-prem/1.8/guide/get-started/introduction.html\")\n",
    "url_waiting_list = url_sets.copy()\n",
    "pages = list()\n",
    "while url_waiting_list:\n",
    "    url = url_waiting_list.pop()\n",
    "    source_html, page_urls = parse_url(url)\n",
    "    pages.append((url, source_html))\n",
    "    new_urls = {url for _, url in page_urls if url not in url_sets}\n",
    "    url_waiting_list.update(new_urls)\n",
    "    url_sets.update(new_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3c8688-9d57-410e-8232-7232257e0417",
   "metadata": {},
   "source": [
    "## Importing Webpage Data into Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ae0143-8ae2-4efe-b46e-f8b165e4be6e",
   "metadata": {},
   "source": [
    "### Using SuperduperDB to Connect to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bedba21d-f2da-4da5-8a73-d091a8130dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhouhaha/workspace/SuperDuperDB/poc-geti/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-02-28 15:00:34,663\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m 2024-Feb-28 15:00:34.67\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36m183eefeaab2d\u001b[0m| \u001b[36m31cafef7-ff65-4fb9-a076-ac55aac52f8c\u001b[0m| \u001b[36msuperduperdb.base.build\u001b[0m:\u001b[36m61  \u001b[0m | \u001b[1mData Client is ready. MongoClient(host=['127.0.0.1:27017'], document_class=dict, tz_aware=False, connect=True, serverselectiontimeoutms=5000)\u001b[0m\n",
      "\u001b[32m 2024-Feb-28 15:00:34.68\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36m183eefeaab2d\u001b[0m| \u001b[36m31cafef7-ff65-4fb9-a076-ac55aac52f8c\u001b[0m| \u001b[36msuperduperdb.base.build\u001b[0m:\u001b[36m36  \u001b[0m | \u001b[1mConnecting to Metadata Client with engine:  MongoClient(host=['127.0.0.1:27017'], document_class=dict, tz_aware=False, connect=True, serverselectiontimeoutms=5000)\u001b[0m\n",
      "\u001b[32m 2024-Feb-28 15:00:34.68\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36m183eefeaab2d\u001b[0m| \u001b[36m31cafef7-ff65-4fb9-a076-ac55aac52f8c\u001b[0m| \u001b[36msuperduperdb.base.build\u001b[0m:\u001b[36m144 \u001b[0m | \u001b[1mConnecting to compute client: local\u001b[0m\n",
      "\u001b[32m 2024-Feb-28 15:00:34.68\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36m183eefeaab2d\u001b[0m| \u001b[36m31cafef7-ff65-4fb9-a076-ac55aac52f8c\u001b[0m| \u001b[36msuperduperdb.base.datalayer\u001b[0m:\u001b[36m80  \u001b[0m | \u001b[1mBuilding Data Layer\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from superduperdb import superduper\n",
    "db = superduper(\"mongodb://127.0.0.1:27017/intel-geti\")\n",
    "db.drop(force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51be29fe-8558-426e-98e2-68045cb4a6fe",
   "metadata": {},
   "source": [
    "Store the webpage data into the database after unstructured parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8c4fb8e-86ea-4bc7-b68c-f5d289a00d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-02-28 15:00:37] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:37] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:37] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:37] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:37] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:37] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:37] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:37] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:37] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:37] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:37] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:37] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:37] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:37] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:37] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:37] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:37] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:37] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:37] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:37] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:38] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:39] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:39] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:39] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:39] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:39] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:39] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:39] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:39] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:39] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:39] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:39] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:39] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:39] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:39] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:39] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:39] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:39] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:39] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:39] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:39] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:39] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:39] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:39] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:39] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:39] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:39] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:39] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:39] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:39] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:39] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:39] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:39] unstructured INFO Reading document ...\n",
      "[2024-02-28 15:00:39] unstructured INFO Reading document from string ...\n",
      "[2024-02-28 15:00:39] unstructured INFO Reading document ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ObjectId('65deda17b9b5b0b929a00663'),\n",
       " ObjectId('65deda17b9b5b0b929a00664'),\n",
       " ObjectId('65deda17b9b5b0b929a00665'),\n",
       " ObjectId('65deda17b9b5b0b929a00666'),\n",
       " ObjectId('65deda17b9b5b0b929a00667'),\n",
       " ObjectId('65deda17b9b5b0b929a00668'),\n",
       " ObjectId('65deda17b9b5b0b929a00669'),\n",
       " ObjectId('65deda17b9b5b0b929a0066a'),\n",
       " ObjectId('65deda17b9b5b0b929a0066b'),\n",
       " ObjectId('65deda17b9b5b0b929a0066c'),\n",
       " ObjectId('65deda17b9b5b0b929a0066d'),\n",
       " ObjectId('65deda17b9b5b0b929a0066e'),\n",
       " ObjectId('65deda17b9b5b0b929a0066f'),\n",
       " ObjectId('65deda17b9b5b0b929a00670'),\n",
       " ObjectId('65deda17b9b5b0b929a00671'),\n",
       " ObjectId('65deda17b9b5b0b929a00672'),\n",
       " ObjectId('65deda17b9b5b0b929a00673'),\n",
       " ObjectId('65deda17b9b5b0b929a00674'),\n",
       " ObjectId('65deda17b9b5b0b929a00675'),\n",
       " ObjectId('65deda17b9b5b0b929a00676'),\n",
       " ObjectId('65deda17b9b5b0b929a00677'),\n",
       " ObjectId('65deda17b9b5b0b929a00678'),\n",
       " ObjectId('65deda17b9b5b0b929a00679'),\n",
       " ObjectId('65deda17b9b5b0b929a0067a'),\n",
       " ObjectId('65deda17b9b5b0b929a0067b'),\n",
       " ObjectId('65deda17b9b5b0b929a0067c'),\n",
       " ObjectId('65deda17b9b5b0b929a0067d'),\n",
       " ObjectId('65deda17b9b5b0b929a0067e'),\n",
       " ObjectId('65deda17b9b5b0b929a0067f'),\n",
       " ObjectId('65deda17b9b5b0b929a00680'),\n",
       " ObjectId('65deda17b9b5b0b929a00681'),\n",
       " ObjectId('65deda17b9b5b0b929a00682'),\n",
       " ObjectId('65deda17b9b5b0b929a00683'),\n",
       " ObjectId('65deda17b9b5b0b929a00684'),\n",
       " ObjectId('65deda17b9b5b0b929a00685'),\n",
       " ObjectId('65deda17b9b5b0b929a00686'),\n",
       " ObjectId('65deda17b9b5b0b929a00687'),\n",
       " ObjectId('65deda17b9b5b0b929a00688'),\n",
       " ObjectId('65deda17b9b5b0b929a00689'),\n",
       " ObjectId('65deda17b9b5b0b929a0068a'),\n",
       " ObjectId('65deda17b9b5b0b929a0068b'),\n",
       " ObjectId('65deda17b9b5b0b929a0068c'),\n",
       " ObjectId('65deda17b9b5b0b929a0068d'),\n",
       " ObjectId('65deda17b9b5b0b929a0068e'),\n",
       " ObjectId('65deda17b9b5b0b929a0068f'),\n",
       " ObjectId('65deda17b9b5b0b929a00690'),\n",
       " ObjectId('65deda17b9b5b0b929a00691'),\n",
       " ObjectId('65deda17b9b5b0b929a00692'),\n",
       " ObjectId('65deda17b9b5b0b929a00693'),\n",
       " ObjectId('65deda17b9b5b0b929a00694'),\n",
       " ObjectId('65deda17b9b5b0b929a00695'),\n",
       " ObjectId('65deda17b9b5b0b929a00696'),\n",
       " ObjectId('65deda17b9b5b0b929a00697'),\n",
       " ObjectId('65deda17b9b5b0b929a00698'),\n",
       " ObjectId('65deda17b9b5b0b929a00699'),\n",
       " ObjectId('65deda17b9b5b0b929a0069a'),\n",
       " ObjectId('65deda17b9b5b0b929a0069b'),\n",
       " ObjectId('65deda17b9b5b0b929a0069c'),\n",
       " ObjectId('65deda17b9b5b0b929a0069d'),\n",
       " ObjectId('65deda17b9b5b0b929a0069e'),\n",
       " ObjectId('65deda17b9b5b0b929a0069f'),\n",
       " ObjectId('65deda17b9b5b0b929a006a0'),\n",
       " ObjectId('65deda17b9b5b0b929a006a1'),\n",
       " ObjectId('65deda17b9b5b0b929a006a2'),\n",
       " ObjectId('65deda17b9b5b0b929a006a3'),\n",
       " ObjectId('65deda17b9b5b0b929a006a4'),\n",
       " ObjectId('65deda17b9b5b0b929a006a5'),\n",
       " ObjectId('65deda17b9b5b0b929a006a6'),\n",
       " ObjectId('65deda17b9b5b0b929a006a7'),\n",
       " ObjectId('65deda17b9b5b0b929a006a8'),\n",
       " ObjectId('65deda17b9b5b0b929a006a9'),\n",
       " ObjectId('65deda17b9b5b0b929a006aa'),\n",
       " ObjectId('65deda17b9b5b0b929a006ab'),\n",
       " ObjectId('65deda17b9b5b0b929a006ac')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unstructured.partition.html import partition_html\n",
    "from superduperdb.ext.unstructured.encoder import unstructured_encoder\n",
    "\n",
    "db.add(unstructured_encoder)\n",
    "\n",
    "datas = []\n",
    "for url, source_html in pages:\n",
    "    elements = partition_html(text=source_html, html_assemble_articles=True)\n",
    "    if elements:\n",
    "        datas.append({'url': url, 'elements': unstructured_encoder(elements)})\n",
    "\n",
    "from superduperdb import Document\n",
    "from superduperdb.backends.mongodb import Collection\n",
    "documents = list(map(Document, datas))\n",
    "collection = Collection(\"pages\")\n",
    "collection.insert_many(documents).execute(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5556877a-3b7e-4f7e-a675-c4b6c82309ad",
   "metadata": {},
   "source": [
    "## Parsing and Chunking Webpages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b0fb34-909d-4a99-903b-06602073a3bc",
   "metadata": {},
   "source": [
    "Define an title ecognition function to be used as chunk identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d37b889-91e0-4394-bc5a-2573564cd91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.documents.elements import ElementType\n",
    "\n",
    "def get_title_data(element):\n",
    "    data = {}\n",
    "    if element.category != ElementType.TITLE:\n",
    "        return data\n",
    "    if 'link_urls' not in element.metadata.to_dict():\n",
    "        return data\n",
    "\n",
    "    if 'category_depth' not in element.metadata.to_dict():\n",
    "        return data\n",
    "\n",
    "    [link_text, *_] = element.metadata.link_texts\n",
    "\n",
    "    if not link_text:\n",
    "        return data\n",
    "\n",
    "    link_urls = element.metadata.link_urls\n",
    "    if not link_urls:\n",
    "        return data\n",
    "    category_depth = element.metadata.category_depth\n",
    "    return {'link': link_urls[0], 'category_depth':category_depth}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62b2c13-caa8-4810-a75d-e36fe2028064",
   "metadata": {},
   "source": [
    "Define conversion methods for different types of text, such as titles, lists, tables, and code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec51fd27-2ef1-48df-add3-83d07b98c2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "def element2text(element):\n",
    "    title_message = get_title_data(element)\n",
    "    text = element.text\n",
    "    if title_message:\n",
    "        title_tags = '#' * (title_message['category_depth'] + 1)\n",
    "        text = title_tags + ' ' + text\n",
    "        text = text.rstrip('#')\n",
    "\n",
    "    elif element.category == ElementType.LIST_ITEM:\n",
    "        text = '- ' + text\n",
    "\n",
    "    elif element.category == ElementType.TABLE:\n",
    "        html = element.metadata.text_as_html\n",
    "        html = html.replace('|', '')\n",
    "        df = pd.read_html(StringIO(html))[0]\n",
    "        text = df.to_markdown(index=False)\n",
    "        text = text + '  \\n'\n",
    "\n",
    "    if text.startswith(\"CODE::\"):\n",
    "        text = f\"```\\n{text[6:]}\\n```\"\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef35cb56-520e-40be-bdd1-f6ee4240f490",
   "metadata": {},
   "source": [
    "Define chunking functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63b09f64-edeb-4b71-8ce1-c13f24ccbec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunk_texts(text, chunk_size=1000, overlap_size=300):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "\n",
    "    while start < len(text):\n",
    "        if chunks:\n",
    "            start -= overlap_size\n",
    "        end = start + chunk_size\n",
    "        end = min(end, len(text))\n",
    "        chunks.append(text[start:end])\n",
    "        start = end\n",
    "        if start >= len(text):\n",
    "            break\n",
    "\n",
    "    return chunks\n",
    "\n",
    "from collections import defaultdict\n",
    "def get_chunks(elements):\n",
    "    chunk_tree = defaultdict(list)\n",
    "    now_depth = -1\n",
    "    now_path = 'root'\n",
    "    for element in elements:\n",
    "        title_data = get_title_data(element)\n",
    "        if not title_data:\n",
    "            chunk_tree[now_path].append(element)\n",
    "        else:\n",
    "            link = title_data['link']\n",
    "            depth = title_data['category_depth']\n",
    "            if depth > now_depth:\n",
    "                now_path = now_path + \"::\" +link\n",
    "            else:\n",
    "                now_path = '::'.join(now_path.split(\"::\")[:depth+1] + [link])\n",
    "            now_depth = depth\n",
    "            chunk_tree[now_path].append(element)\n",
    "     \n",
    "    chunks = []\n",
    "    for node_path, node_elements in chunk_tree.items():\n",
    "        new_elements = []\n",
    "        nodes = node_path.split(\"::\")\n",
    "        parent_elements = []\n",
    "        for i in range(1, len(nodes) - 1):\n",
    "            [parent_element, *_] = chunk_tree[\"::\".join(nodes[:i+1])] or [None]\n",
    "            if parent_element:\n",
    "                parent_elements.append(parent_element)\n",
    "        node_elements = [*parent_elements, *node_elements]\n",
    "        content = '\\n\\n'.join(map(lambda x: element2text(x), node_elements))\n",
    "        for chunk_text in get_chunk_texts(content):\n",
    "            # The url field is used to save the jump link\n",
    "            # The text field is used for vector search\n",
    "            # The content field is used to submit to LLM for answer\n",
    "            chunk = {\"url\": nodes[-1], 'text': chunk_text, 'content': content}\n",
    "            chunks.append(chunk)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ffc9e0-6148-4886-bd5d-b2ed43364364",
   "metadata": {},
   "source": [
    "Define a chunking model and add a Listener to listen to data and chunk webpages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "606472b7-cda9-454e-9c9d-8ee5f122bade",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "74it [00:00, 33500.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m 2024-Feb-28 15:00:40.10\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36m183eefeaab2d\u001b[0m| \u001b[36m31cafef7-ff65-4fb9-a076-ac55aac52f8c\u001b[0m| \u001b[36msuperduperdb.components.model\u001b[0m:\u001b[36m477 \u001b[0m | \u001b[1mAdding 74 model outputs to `db`\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([None],\n",
       " Listener(identifier='chunk/elements', key='elements', model=Model(identifier='chunk', encoder=None, output_schema=Schema(identifier='myschema', fields={'text': 'string', '_fold': FieldType(identifier='String')}), flatten=True, preprocess=None, postprocess=None, collate_fn=None, batch_predict=False, takes_context=False, metrics=(), model_update_kwargs={'document_embedded': False}, validation_sets=None, predict_X=None, predict_select=None, predict_max_chunk_size=None, predict_kwargs=None, object=<Artifact artifact=<function get_chunks at 0x2956a37f0> serializer=dill>, model_to_device_method=None, metric_values={}, predict_method=None, serializer='dill', device='cpu', preferred_devices=('cuda', 'mps', 'cpu'), training_configuration=None, train_X=None, train_y=None, train_select=None), select=<superduperdb.backends.mongodb.query.MongoCompoundSelect[\n",
       "     \u001b[92m\u001b[1mpages.find({'_id': \"{'$in': '[65deda17b9b5b0b929a00663, 65deda17b9b5b0b929a00664, 65deda17b9b5b0b929a00665, 65deda17b9b5b0b929a00666, 65deda17b9b5b0b929a00667, 65deda17b9b5b0b929a00668, 65deda17b9b5b0b929a00669, 65deda17b9b5b0b929a0066a, 65deda17b9b5b0b929a0066b, 65deda17b9b5b0b929a0066c, 65deda17b9b5b0b929a0066d, 65deda17b9b5b0b929a0066e, 65deda17b9b5b0b929a0066f, 65deda17b9b5b0b929a00670, 65deda17b9b5b0b929a00671, 65deda17b9b5b0b929a00672, 65deda17b9b5b0b929a00673, 65deda17b9b5b0b929a00674, 65deda17b9b5b0b929a00675, 65deda17b9b5b0b929a00676, 65deda17b9b5b0b929a00677, 65deda17b9b5b0b929a00678, 65deda17b9b5b0b929a00679, 65deda17b9b5b0b929a0067a, 65deda17b9b5b0b929a0067b, 65deda17b9b5b0b929a0067c, 65deda17b9b5b0b929a0067d, 65deda17b9b5b0b929a0067e, 65deda17b9b5b0b929a0067f, 65deda17b9b5b0b929a00680, 65deda17b9b5b0b929a00681, 65deda17b9b5b0b929a00682, 65deda17b9b5b0b929a00683, 65deda17b9b5b0b929a00684, 65deda17b9b5b0b929a00685, 65deda17b9b5b0b929a00686, 65deda17b9b5b0b929a00687, 65deda17b9b5b0b929a00688, 65deda17b9b5b0b929a00689, 65deda17b9b5b0b929a0068a, 65deda17b9b5b0b929a0068b, 65deda17b9b5b0b929a0068c, 65deda17b9b5b0b929a0068d, 65deda17b9b5b0b929a0068e, 65deda17b9b5b0b929a0068f, 65deda17b9b5b0b929a00690, 65deda17b9b5b0b929a00691, 65deda17b9b5b0b929a00692, 65deda17b9b5b0b929a00693, 65deda17b9b5b0b929a00694, 65deda17b9b5b0b929a00695, 65deda17b9b5b0b929a00696, 65deda17b9b5b0b929a00697, 65deda17b9b5b0b929a00698, 65deda17b9b5b0b929a00699, 65deda17b9b5b0b929a0069a, 65deda17b9b5b0b929a0069b, 65deda17b9b5b0b929a0069c, 65deda17b9b5b0b929a0069d, 65deda17b9b5b0b929a0069e, 65deda17b9b5b0b929a0069f, 65deda17b9b5b0b929a006a0, 65deda17b9b5b0b929a006a1, 65deda17b9b5b0b929a006a2, 65deda17b9b5b0b929a006a3, 65deda17b9b5b0b929a006a4, 65deda17b9b5b0b929a006a5, 65deda17b9b5b0b929a006a6, 65deda17b9b5b0b929a006a7, 65deda17b9b5b0b929a006a8, 65deda17b9b5b0b929a006a9, 65deda17b9b5b0b929a006aa, 65deda17b9b5b0b929a006ab, 65deda17b9b5b0b929a006ac]'}\"}, {})\u001b[0m\n",
       " ] object at 0x29611cac0>, active=True, predict_kwargs={}))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from superduperdb import Model, Listener, Schema\n",
    "\n",
    "\n",
    "chunk_model = Model(\n",
    "    identifier='chunk',\n",
    "    object=get_chunks,\n",
    "    flatten=True,\n",
    "    model_update_kwargs={\"document_embedded\": False},\n",
    "    output_schema=Schema(identifier=\"myschema\", fields={\"text\": \"string\"}),\n",
    ")\n",
    "\n",
    "db.add(\n",
    "    Listener(\n",
    "        model=chunk_model,\n",
    "        select=Collection('pages').find(),\n",
    "        key=\"elements\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe6130a-03e3-4788-9be9-014386ad7fb1",
   "metadata": {},
   "source": [
    "## Building Vector Search Feature Using OpenAIEmbedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "122cb103-30de-414e-b36f-b2a85fbcd1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-02-28 15:00:41] httpx INFO HTTP Request: GET https://api.openai.com/v1/models \"HTTP/1.1 200 OK\"\n",
      "983it [00:00, 137506.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m 2024-Feb-28 15:00:41.36\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36m183eefeaab2d\u001b[0m| \u001b[36m31cafef7-ff65-4fb9-a076-ac55aac52f8c\u001b[0m| \u001b[36msuperduperdb.components.model\u001b[0m:\u001b[36m417 \u001b[0m | \u001b[1mComputing chunk 0/15\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                          | 0/1 [00:00<?, ?it/s][2024-02-28 15:00:42] httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m 2024-Feb-28 15:00:43.11\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36m183eefeaab2d\u001b[0m| \u001b[36m31cafef7-ff65-4fb9-a076-ac55aac52f8c\u001b[0m| \u001b[36msuperduperdb.components.model\u001b[0m:\u001b[36m477 \u001b[0m | \u001b[1mAdding 64 model outputs to `db`\u001b[0m\n",
      "\u001b[32m 2024-Feb-28 15:00:43.20\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36m183eefeaab2d\u001b[0m| \u001b[36m31cafef7-ff65-4fb9-a076-ac55aac52f8c\u001b[0m| \u001b[36msuperduperdb.components.model\u001b[0m:\u001b[36m417 \u001b[0m | \u001b[1mComputing chunk 1/15\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                          | 0/1 [00:00<?, ?it/s][2024-02-28 15:00:43] httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m 2024-Feb-28 15:00:44.41\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36m183eefeaab2d\u001b[0m| \u001b[36m31cafef7-ff65-4fb9-a076-ac55aac52f8c\u001b[0m| \u001b[36msuperduperdb.components.model\u001b[0m:\u001b[36m477 \u001b[0m | \u001b[1mAdding 64 model outputs to `db`\u001b[0m\n",
      "\u001b[32m 2024-Feb-28 15:00:44.48\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36m183eefeaab2d\u001b[0m| \u001b[36m31cafef7-ff65-4fb9-a076-ac55aac52f8c\u001b[0m| \u001b[36msuperduperdb.components.model\u001b[0m:\u001b[36m417 \u001b[0m | \u001b[1mComputing chunk 2/15\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                          | 0/1 [00:00<?, ?it/s][2024-02-28 15:00:44] httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m 2024-Feb-28 15:00:45.15\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36m183eefeaab2d\u001b[0m| \u001b[36m31cafef7-ff65-4fb9-a076-ac55aac52f8c\u001b[0m| \u001b[36msuperduperdb.components.model\u001b[0m:\u001b[36m477 \u001b[0m | \u001b[1mAdding 64 model outputs to `db`\u001b[0m\n",
      "\u001b[32m 2024-Feb-28 15:00:45.25\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36m183eefeaab2d\u001b[0m| \u001b[36m31cafef7-ff65-4fb9-a076-ac55aac52f8c\u001b[0m| \u001b[36msuperduperdb.components.model\u001b[0m:\u001b[36m417 \u001b[0m | \u001b[1mComputing chunk 3/15\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                          | 0/1 [00:00<?, ?it/s][2024-02-28 15:00:45] httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m 2024-Feb-28 15:00:45.89\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36m183eefeaab2d\u001b[0m| \u001b[36m31cafef7-ff65-4fb9-a076-ac55aac52f8c\u001b[0m| \u001b[36msuperduperdb.components.model\u001b[0m:\u001b[36m477 \u001b[0m | \u001b[1mAdding 64 model outputs to `db`\u001b[0m\n",
      "\u001b[32m 2024-Feb-28 15:00:45.96\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36m183eefeaab2d\u001b[0m| \u001b[36m31cafef7-ff65-4fb9-a076-ac55aac52f8c\u001b[0m| \u001b[36msuperduperdb.components.model\u001b[0m:\u001b[36m417 \u001b[0m | \u001b[1mComputing chunk 4/15\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                          | 0/1 [00:00<?, ?it/s][2024-02-28 15:00:46] httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m 2024-Feb-28 15:00:46.64\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36m183eefeaab2d\u001b[0m| \u001b[36m31cafef7-ff65-4fb9-a076-ac55aac52f8c\u001b[0m| \u001b[36msuperduperdb.components.model\u001b[0m:\u001b[36m477 \u001b[0m | \u001b[1mAdding 64 model outputs to `db`\u001b[0m\n",
      "\u001b[32m 2024-Feb-28 15:00:46.77\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36m183eefeaab2d\u001b[0m| \u001b[36m31cafef7-ff65-4fb9-a076-ac55aac52f8c\u001b[0m| \u001b[36msuperduperdb.components.model\u001b[0m:\u001b[36m417 \u001b[0m | \u001b[1mComputing chunk 5/15\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                          | 0/1 [00:00<?, ?it/s][2024-02-28 15:00:47] httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m 2024-Feb-28 15:00:47.52\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36m183eefeaab2d\u001b[0m| \u001b[36m31cafef7-ff65-4fb9-a076-ac55aac52f8c\u001b[0m| \u001b[36msuperduperdb.components.model\u001b[0m:\u001b[36m477 \u001b[0m | \u001b[1mAdding 64 model outputs to `db`\u001b[0m\n",
      "\u001b[32m 2024-Feb-28 15:00:47.58\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36m183eefeaab2d\u001b[0m| \u001b[36m31cafef7-ff65-4fb9-a076-ac55aac52f8c\u001b[0m| \u001b[36msuperduperdb.components.model\u001b[0m:\u001b[36m417 \u001b[0m | \u001b[1mComputing chunk 6/15\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                          | 0/1 [00:00<?, ?it/s][2024-02-28 15:00:48] httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m 2024-Feb-28 15:00:48.33\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36m183eefeaab2d\u001b[0m| \u001b[36m31cafef7-ff65-4fb9-a076-ac55aac52f8c\u001b[0m| \u001b[36msuperduperdb.components.model\u001b[0m:\u001b[36m477 \u001b[0m | \u001b[1mAdding 64 model outputs to `db`\u001b[0m\n",
      "\u001b[32m 2024-Feb-28 15:00:48.39\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36m183eefeaab2d\u001b[0m| \u001b[36m31cafef7-ff65-4fb9-a076-ac55aac52f8c\u001b[0m| \u001b[36msuperduperdb.components.model\u001b[0m:\u001b[36m417 \u001b[0m | \u001b[1mComputing chunk 7/15\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                          | 0/1 [00:00<?, ?it/s][2024-02-28 15:00:48] httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m 2024-Feb-28 15:00:49.20\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36m183eefeaab2d\u001b[0m| \u001b[36m31cafef7-ff65-4fb9-a076-ac55aac52f8c\u001b[0m| \u001b[36msuperduperdb.components.model\u001b[0m:\u001b[36m477 \u001b[0m | \u001b[1mAdding 64 model outputs to `db`\u001b[0m\n",
      "\u001b[32m 2024-Feb-28 15:00:49.26\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36m183eefeaab2d\u001b[0m| \u001b[36m31cafef7-ff65-4fb9-a076-ac55aac52f8c\u001b[0m| \u001b[36msuperduperdb.components.model\u001b[0m:\u001b[36m417 \u001b[0m | \u001b[1mComputing chunk 8/15\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                          | 0/1 [00:00<?, ?it/s][2024-02-28 15:00:49] httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m 2024-Feb-28 15:00:50.03\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36m183eefeaab2d\u001b[0m| \u001b[36m31cafef7-ff65-4fb9-a076-ac55aac52f8c\u001b[0m| \u001b[36msuperduperdb.components.model\u001b[0m:\u001b[36m477 \u001b[0m | \u001b[1mAdding 64 model outputs to `db`\u001b[0m\n",
      "\u001b[32m 2024-Feb-28 15:00:50.10\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36m183eefeaab2d\u001b[0m| \u001b[36m31cafef7-ff65-4fb9-a076-ac55aac52f8c\u001b[0m| \u001b[36msuperduperdb.components.model\u001b[0m:\u001b[36m417 \u001b[0m | \u001b[1mComputing chunk 9/15\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                          | 0/1 [00:00<?, ?it/s][2024-02-28 15:00:50] httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m 2024-Feb-28 15:00:51.01\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36m183eefeaab2d\u001b[0m| \u001b[36m31cafef7-ff65-4fb9-a076-ac55aac52f8c\u001b[0m| \u001b[36msuperduperdb.components.model\u001b[0m:\u001b[36m477 \u001b[0m | \u001b[1mAdding 64 model outputs to `db`\u001b[0m\n",
      "\u001b[32m 2024-Feb-28 15:00:51.07\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36m183eefeaab2d\u001b[0m| \u001b[36m31cafef7-ff65-4fb9-a076-ac55aac52f8c\u001b[0m| \u001b[36msuperduperdb.components.model\u001b[0m:\u001b[36m417 \u001b[0m | \u001b[1mComputing chunk 10/15\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                          | 0/1 [00:00<?, ?it/s][2024-02-28 15:00:51] httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m 2024-Feb-28 15:00:51.97\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36m183eefeaab2d\u001b[0m| \u001b[36m31cafef7-ff65-4fb9-a076-ac55aac52f8c\u001b[0m| \u001b[36msuperduperdb.components.model\u001b[0m:\u001b[36m477 \u001b[0m | \u001b[1mAdding 64 model outputs to `db`\u001b[0m\n",
      "\u001b[32m 2024-Feb-28 15:00:52.06\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36m183eefeaab2d\u001b[0m| \u001b[36m31cafef7-ff65-4fb9-a076-ac55aac52f8c\u001b[0m| \u001b[36msuperduperdb.components.model\u001b[0m:\u001b[36m417 \u001b[0m | \u001b[1mComputing chunk 11/15\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                          | 0/1 [00:00<?, ?it/s][2024-02-28 15:00:52] httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m 2024-Feb-28 15:00:52.72\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36m183eefeaab2d\u001b[0m| \u001b[36m31cafef7-ff65-4fb9-a076-ac55aac52f8c\u001b[0m| \u001b[36msuperduperdb.components.model\u001b[0m:\u001b[36m477 \u001b[0m | \u001b[1mAdding 64 model outputs to `db`\u001b[0m\n",
      "\u001b[32m 2024-Feb-28 15:00:52.80\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36m183eefeaab2d\u001b[0m| \u001b[36m31cafef7-ff65-4fb9-a076-ac55aac52f8c\u001b[0m| \u001b[36msuperduperdb.components.model\u001b[0m:\u001b[36m417 \u001b[0m | \u001b[1mComputing chunk 12/15\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                          | 0/1 [00:00<?, ?it/s][2024-02-28 15:00:53] httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m 2024-Feb-28 15:00:54.12\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36m183eefeaab2d\u001b[0m| \u001b[36m31cafef7-ff65-4fb9-a076-ac55aac52f8c\u001b[0m| \u001b[36msuperduperdb.components.model\u001b[0m:\u001b[36m477 \u001b[0m | \u001b[1mAdding 64 model outputs to `db`\u001b[0m\n",
      "\u001b[32m 2024-Feb-28 15:00:54.18\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36m183eefeaab2d\u001b[0m| \u001b[36m31cafef7-ff65-4fb9-a076-ac55aac52f8c\u001b[0m| \u001b[36msuperduperdb.components.model\u001b[0m:\u001b[36m417 \u001b[0m | \u001b[1mComputing chunk 13/15\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                          | 0/1 [00:00<?, ?it/s][2024-02-28 15:00:54] httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m 2024-Feb-28 15:00:54.96\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36m183eefeaab2d\u001b[0m| \u001b[36m31cafef7-ff65-4fb9-a076-ac55aac52f8c\u001b[0m| \u001b[36msuperduperdb.components.model\u001b[0m:\u001b[36m477 \u001b[0m | \u001b[1mAdding 64 model outputs to `db`\u001b[0m\n",
      "\u001b[32m 2024-Feb-28 15:00:55.02\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36m183eefeaab2d\u001b[0m| \u001b[36m31cafef7-ff65-4fb9-a076-ac55aac52f8c\u001b[0m| \u001b[36msuperduperdb.components.model\u001b[0m:\u001b[36m417 \u001b[0m | \u001b[1mComputing chunk 14/15\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                          | 0/1 [00:00<?, ?it/s][2024-02-28 15:00:55] httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m 2024-Feb-28 15:00:55.94\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36m183eefeaab2d\u001b[0m| \u001b[36m31cafef7-ff65-4fb9-a076-ac55aac52f8c\u001b[0m| \u001b[36msuperduperdb.components.model\u001b[0m:\u001b[36m477 \u001b[0m | \u001b[1mAdding 64 model outputs to `db`\u001b[0m\n",
      "\u001b[32m 2024-Feb-28 15:00:56.01\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36m183eefeaab2d\u001b[0m| \u001b[36m31cafef7-ff65-4fb9-a076-ac55aac52f8c\u001b[0m| \u001b[36msuperduperdb.components.model\u001b[0m:\u001b[36m417 \u001b[0m | \u001b[1mComputing chunk 15/15\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                          | 0/1 [00:00<?, ?it/s][2024-02-28 15:00:56] httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m 2024-Feb-28 15:00:56.49\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36m183eefeaab2d\u001b[0m| \u001b[36m31cafef7-ff65-4fb9-a076-ac55aac52f8c\u001b[0m| \u001b[36msuperduperdb.components.model\u001b[0m:\u001b[36m477 \u001b[0m | \u001b[1mAdding 23 model outputs to `db`\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([None],\n",
       " VectorIndex(identifier='vector_index', indexing_listener=Listener(identifier='text-embedding-ada-002/elements', key='_outputs.elements.chunk', model=OpenAIEmbedding(encoder=Encoder(identifier='vector[1536]', decoder=None, encoder=None, shape=(1536,), load_hybrid=True), output_schema=None, flatten=False, preprocess=<Artifact artifact=<function preprocess at 0x2960ed2d0> serializer=dill>, postprocess=None, collate_fn=None, batch_predict=False, takes_context=False, metrics=(), model_update_kwargs={}, validation_sets=None, predict_X=None, predict_select=None, predict_max_chunk_size=None, predict_kwargs=None, identifier='text-embedding-ada-002', model='text-embedding-ada-002', client_kwargs={}, shape=(1536,)), select=<superduperdb.backends.mongodb.query.MongoCompoundSelect[\n",
       "     \u001b[92m\u001b[1m_outputs.elements.chunk.elements.chunk.find({'_id': \"{'$in': '[65deda18b9b5b0b929a00a73, 65deda18b9b5b0b929a00a74, 65deda18b9b5b0b929a00a75, 65deda18b9b5b0b929a00a76, 65deda18b9b5b0b929a00a77, 65deda18b9b5b0b929a00a78, 65deda18b9b5b0b929a00a79, 65deda18b9b5b0b929a00a7a, 65deda18b9b5b0b929a00a7b, 65deda18b9b5b0b929a00a7c, 65deda18b9b5b0b929a00a7d, 65deda18b9b5b0b929a00a7e, 65deda18b9b5b0b929a00a7f, 65deda18b9b5b0b929a00a80, 65deda18b9b5b0b929a00a81, 65deda18b9b5b0b929a00a82, 65deda18b9b5b0b929a00a83, 65deda18b9b5b0b929a00a84, 65deda18b9b5b0b929a00a85, 65deda18b9b5b0b929a00a86, 65deda18b9b5b0b929a00a87, 65deda18b9b5b0b929a00a88, 65deda18b9b5b0b929a00a89]'}\"}, {})\u001b[0m\n",
       " ] object at 0x297f6c880>, active=True, predict_kwargs={'max_chunk_size': 64}), compatible_listener=None, measure=<VectorIndexMeasureType.cosine: 'cosine'>, metric_values={}))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from superduperdb.ext.openai import OpenAIEmbedding\n",
    "from superduperdb.base.artifact import Artifact\n",
    "from tqdm import tqdm\n",
    "def _predict(self, X, one: bool = False, **kwargs):\n",
    "    if isinstance(X, str) or one:\n",
    "        if isinstance(self.preprocess, Artifact):\n",
    "            X = self.preprocess.artifact(X)\n",
    "        return self._predict_one(X)\n",
    "\n",
    "    if isinstance(self.preprocess, Artifact):\n",
    "        X = [self.preprocess.artifact(i) for i in X]\n",
    "\n",
    "    out = []\n",
    "    batch_size = kwargs.pop(\"batch_size\", 100)\n",
    "    for i in tqdm(range(0, len(X), batch_size)):\n",
    "        out.extend(self._predict_a_batch(X[i : i + batch_size], **kwargs))\n",
    "    return out\n",
    "\n",
    "\n",
    "OpenAIEmbedding._predict = _predict\n",
    "\n",
    "from superduperdb.ext.openai import OpenAIEmbedding\n",
    "from superduperdb.base.artifact import Artifact\n",
    "from superduperdb import VectorIndex\n",
    "\n",
    "def preprocess(x):\n",
    "    if isinstance(x, dict):\n",
    "        # For model chains, the logic of this key needs to be optimized.\n",
    "        chunk = sorted(x.items())[-1][1]\n",
    "        return chunk[\"text\"]\n",
    "    return x\n",
    "\n",
    "# Create an instance of the OpenAIEmbedding model with the specified identifier ('text-embedding-ada-002')\n",
    "model = OpenAIEmbedding(\n",
    "    identifier='text-embedding-ada-002',\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    preprocess=Artifact(preprocess),\n",
    ")\n",
    "\n",
    "\n",
    "db.add(\n",
    "    VectorIndex(\n",
    "        identifier='vector_index',\n",
    "        indexing_listener=Listener(\n",
    "            select=Collection('_outputs.elements.chunk').find(),\n",
    "            key='_outputs.elements.chunk',  # Key for the documents\n",
    "            model=model,  # Specify the model for processing\n",
    "            predict_kwargs={\"max_chunk_size\": 64},\n",
    "        ),\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4477b3-eab2-48b5-bbe9-e223c80646dc",
   "metadata": {},
   "source": [
    "Define a function for vector search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0250b6aa-4fcc-464d-87e8-e6a911810a11",
   "metadata": {},
   "source": [
    "# Create vector search and Chatbot applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07f1b885-ff42-45dc-be2e-649ab2b674dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m 2024-Feb-28 15:00:56.59\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36m183eefeaab2d\u001b[0m| \u001b[36m31cafef7-ff65-4fb9-a076-ac55aac52f8c\u001b[0m| \u001b[36msuperduperdb.base.build\u001b[0m:\u001b[36m61  \u001b[0m | \u001b[1mData Client is ready. MongoClient(host=['127.0.0.1:27017'], document_class=dict, tz_aware=False, connect=True, serverselectiontimeoutms=5000)\u001b[0m\n",
      "\u001b[32m 2024-Feb-28 15:00:56.59\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36m183eefeaab2d\u001b[0m| \u001b[36m31cafef7-ff65-4fb9-a076-ac55aac52f8c\u001b[0m| \u001b[36msuperduperdb.base.build\u001b[0m:\u001b[36m36  \u001b[0m | \u001b[1mConnecting to Metadata Client with engine:  MongoClient(host=['127.0.0.1:27017'], document_class=dict, tz_aware=False, connect=True, serverselectiontimeoutms=5000)\u001b[0m\n",
      "\u001b[32m 2024-Feb-28 15:00:56.59\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36m183eefeaab2d\u001b[0m| \u001b[36m31cafef7-ff65-4fb9-a076-ac55aac52f8c\u001b[0m| \u001b[36msuperduperdb.base.build\u001b[0m:\u001b[36m144 \u001b[0m | \u001b[1mConnecting to compute client: local\u001b[0m\n",
      "\u001b[32m 2024-Feb-28 15:00:56.59\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36m183eefeaab2d\u001b[0m| \u001b[36m31cafef7-ff65-4fb9-a076-ac55aac52f8c\u001b[0m| \u001b[36msuperduperdb.base.datalayer\u001b[0m:\u001b[36m80  \u001b[0m | \u001b[1mBuilding Data Layer\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "db = superduper(\"mongodb://127.0.0.1:27017/intel-geti\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06c92c51-f81d-4b70-93fc-281b0849f600",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_search(db, query, top_k=5):\n",
    "    logging.info(f\"Vector search query: {query}\")\n",
    "    collection = Collection('_outputs.elements.chunk')\n",
    "    outs = db.execute(\n",
    "        collection.like(\n",
    "            Document({\"_outputs.elements.chunk\": query}),\n",
    "            vector_index=\"vector_index\",\n",
    "            n=top_k,\n",
    "        ).find({})\n",
    "    )\n",
    "    if outs:\n",
    "        outs = sorted(outs, key=lambda x: x.content[\"score\"], reverse=True)\n",
    "    for out in outs:\n",
    "        print(\"-\" * 20, '\\n')\n",
    "        data = out.outputs(\"elements\", 'chunk')\n",
    "    \n",
    "        source = out.content['_source']\n",
    "        source_url = Collection('pages').find_one({\"_id\": source}).execute(db)['url']\n",
    "        data = out.outputs(\"elements\", 'chunk')\n",
    "        url = source_url + data['url']\n",
    "        print(url, out['score'])\n",
    "        print(data[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "372fb2c1-1de1-482e-baee-1553b4f72c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-02-28 15:00:56] root INFO Vector search query: What parameters does the DeployedModel class have?\n",
      "[2024-02-28 15:00:57] httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m 2024-Feb-28 15:00:57.39\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36m183eefeaab2d\u001b[0m| \u001b[36m31cafef7-ff65-4fb9-a076-ac55aac52f8c\u001b[0m| \u001b[36msuperduperdb.base.datalayer\u001b[0m:\u001b[36m154 \u001b[0m | \u001b[1mloading of vectors of vector-index: 'vector_index'\u001b[0m\n",
      "\u001b[32m 2024-Feb-28 15:00:57.39\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36m183eefeaab2d\u001b[0m| \u001b[36m31cafef7-ff65-4fb9-a076-ac55aac52f8c\u001b[0m| \u001b[36msuperduperdb.base.datalayer\u001b[0m:\u001b[36m170 \u001b[0m | \u001b[1m<superduperdb.backends.mongodb.query.MongoCompoundSelect[\n",
      "    \u001b[92m\u001b[1m_outputs.elements.chunk.elements.chunk.find({}, {'_outputs.elements.text-embedding-ada-002.0': '1', '_outputs.elements.text-embedding-ada-002/0': '1', '_id': '1'})\u001b[0m\n",
      "] object at 0x297fb9f30>\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading vectors into vector-table...: 983it [00:00, 1124.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- \n",
      "\n",
      "https://openvinotoolkit.github.io/geti-sdk/geti_sdk.data_models.html#deployment-related-entities 0.8028081338966921\n",
      "# geti_sdk.data_models\n",
      "\n",
      "## Module contents\n",
      "\n",
      "### Deployment-related entities\n",
      "-------------------- \n",
      "\n",
      "https://docs.geti.intel.com/on-prem/1.8/guide/deployments/deployments.html#deployments 0.8019525830346235\n",
      "# Deployments\n",
      "\n",
      "Important\n",
      "\n",
      "The expected color code is RGB for IntelÂ® Getiâ¢ exportable code and deployment.\n",
      "\n",
      "The Deployments screen allows users to export deployment code for the trained models.\n",
      "\n",
      "Once you have tested and optimized the model, you are ready to download and deploy your solution. To download the code, click on Select model for deployment. In the dialog box, choose the model architecture and its version as well as the model optimization if available. In the case of a task chain project, you will need to select two models through a wizard. Upon your selection, click Download.\n",
      "-------------------- \n",
      "\n",
      "https://openvinotoolkit.github.io/geti-sdk/geti_sdk.deployment.html#geti_sdk.deployment.deployed_model.DeployedModel 0.7917182898537618\n",
      "# geti_sdk.deployment\n",
      "\n",
      "## Module contents\n",
      "\n",
      "### Class: geti_sdk.deployment.deployed_model.DeployedModel\n",
      "\n",
      "class geti_sdk.deployment.deployed_model.DeployedModel(name: str, fps_throughput: str, latency: str, precision: List[str], creation_date: str | datetime | None, size: int | None = None, target_device: str | None = None, target_device_type: str | None = None, previous_revision_id: str | None = None, previous_trained_revision_id: str | None = None, score: float | None = None, performance: Performance | None = None, id: str | None = None, label_schema_in_sync: bool | None = None, model_format: str | None = None, has_xai_head: bool = False, *, model_status: str | EnumType, optimization_methods: List[str], optimization_objectives: Dict[str, Any], optimization_type: str | EnumType, version: int | None = None, configurations: List[OptimizationConfigurationParameter] | None = None, hyper_parameters: TaskConfiguration | None = None)\n",
      "\n",
      "- Bases: OptimizedModel\n",
      "Representation of an Intel® Geti™ model that has been deployed for inference. It\n",
      "can be loaded onto a device to generate predictions.\n",
      "\n",
      "\n",
      "hyper_parameters: TaskConfiguration | None\n",
      "\n",
      "\n",
      "\n",
      "property model_data_path: str\n",
      "Return the path to the raw model data\n",
      "\n",
      "Returns:\n",
      "path to the directory containing the raw model data\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "get_data(source: str | PathLike | GetiSession)\n",
      "Load the model weights from a data source. The source can be one of the\n",
      "following:\n",
      "\n",
      "\n",
      "The Intel® Geti™ platform (if an GetiSession instance is passed). In this\n",
      "\n",
      "\n",
      "case the weights will be downloaded, and extracted to a temporary directory\n",
      "\n",
      "\n",
      "A zip file on local disk, in this case the weights will be extracted to a\n",
      "temporary directory\n",
      "A folder on local disk containing the .xml and .bin file for the model\n",
      "\n",
      "\n",
      "\n",
      "Parameters:\n",
      "source – Data source to load the weights from\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "load_inference_model(device: str = 'CPU', configuration: Dict[str, Any] | None = None, project: Project | None = None) → None\n",
      "Load the actual model weights to a specified device.\n",
      "\n",
      "Parameters:\n",
      "\n",
      "device – Device (CPU or GPU) to load the model to. Defaults to ‘CPU’\n",
      "configuration – Optional dictionary holding additional configuration\n",
      "parameters for the model\n",
      "project – Optional project to which the model belongs.\n",
      "This is only used when the model is run on OVMS, in that case the\n",
      "project is needed to identify the correct model\n",
      "\n",
      "\n",
      "Returns:\n",
      "OpenVino inference engine model that can be used to make predictions\n",
      "on images\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "classmethod from_model_and_hypers(model: OptimizedModel, hyper_parameters: TaskConfiguration | None = None) → DeployedModel\n",
      "Create a DeployedModel instance out of an OptimizedModel and it’s\n",
      "corresponding set of hyper parameters.\n",
      "\n",
      "Parameters:\n",
      "\n",
      "model – OptimizedModel to convert to a DeployedModel\n",
      "hyper_parameters – TaskConfiguration instance containing the hyper\n",
      "parameters for the model\n",
      "\n",
      "\n",
      "Returns:\n",
      "DeployedModel instance\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "classmethod from_folder(path_to_folder: str | PathLike) → DeployedModel\n",
      "Create a DeployedModel instance from a folder containing the model data.\n",
      "\n",
      "Parameters:\n",
      "path_to_folder – Path to the folder that holds the model data\n",
      "\n",
      "Returns:\n",
      "DeployedModel instance\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "save(path_to_folder: str | PathLike) → bool\n",
      "Save the DeployedModel instance to the designated folder.\n",
      "\n",
      "Parameters:\n",
      "path_to_folder – Path to the folder to save the model to\n",
      "\n",
      "Returns:\n",
      "True if the model was saved successfully, False otherwise\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "preprocess(image: ndarray) → Tuple[Dict[str, ndarray], Dict[str, Tuple[int, int, int]]]\n",
      "Preprocess an image for inference.\n",
      "\n",
      "Parameters:\n",
      "image – Numpy array containing pixel data. The image is expected to have\n",
      "dimensions [height x width x channels], with the channels in RGB order\n",
      "\n",
      "Returns:\n",
      "Dictionary holding the preprocessing result, the original shape of\n",
      "the image and the shape after preprocessing\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "postprocess(inference_results: Dict[str, ndarray], metadata: Dict[str, Any] | None = None) → ndarray | List[Tuple[int, float]] | Any\n",
      "Postprocess the model outputs.\n",
      "\n",
      "Parameters:\n",
      "\n",
      "inference_results – Dictionary holding the results of inference\n",
      "metadata – Dictionary holding metadata\n",
      "\n",
      "\n",
      "Returns:\n",
      "Postprocessed inference results. The exact format depends on the\n",
      "type of model that is loaded:\n",
      "\n",
      "For segmentation models, it will be a numpy array holding the output mask\n",
      "\n",
      "For classification models, it will be a list of tuples holding theoutput class index and class probability\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "For detection models, it will be an instance ofopenvino.model_zoo.model_api.models.utils.Detection, holding the\n",
      "bounding box output\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "postprocess_explain_outputs(inference_results: Dict[str, ndarray], metadata: Dict[str, Any] | None = None) → Tuple[ndarray, ndarray]\n",
      "Postprocess the model outputs to obtain saliency maps, feature vectors and\n",
      "active scores.\n",
      "\n",
      "Parameters:\n",
      "\n",
      "inference_results – Dictionary holding the results of inference\n",
      "metadata – Dictionary holding metadata\n",
      "\n",
      "\n",
      "Returns:\n",
      "Tuple containing postprocessed outputs, formatted as follows:\n",
      "- Numpy array containing the saliency map\n",
      "- Numpy array containing the feature vector\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "infer(preprocessed_image: Dict[str, ndarray]) → Dict[str, ndarray]\n",
      "Run inference on an already preprocessed image.\n",
      "\n",
      "Parameters:\n",
      "preprocessed_image – Dictionary holding the preprocessing results for an\n",
      "image\n",
      "\n",
      "Returns:\n",
      "Dictionary containing the model outputs\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "property ote_label_schema: LabelSchemaEntity\n",
      "Return the OTE LabelSchema for the model.\n",
      "This requires the inference model to be loaded, getting this property while\n",
      "inference models are not loaded will raise a ValueError\n",
      "\n",
      "Returns:\n",
      "LabelSchemaEntity containing the OTE SDK label schema for the model\n",
      "-------------------- \n",
      "\n",
      "https://docs.geti.intel.com/on-prem/1.8/guide/release-notes/1.8/release-1.8.html#new-default-model-deployment 0.7895183813853132\n",
      "# IntelÂ® Getiâ¢ 1.8.0\n",
      "\n",
      "## Release Details\n",
      "\n",
      "### New default model deployment\n",
      "\n",
      "To further improve user experience for model deployment, the default deployment option in the platform is now set to SDK deployment, instead of utilizing the exportable code. This enables users to take advantage of the SDK functionalities to build inference pipelines directly, rather than just running live inference on their device with the exportable code.\n",
      "\n",
      "Key changes:\n",
      "\n",
      "- On the Models screen, only the raw models will now be exportable.\n",
      "\n",
      "- The Deployment screen remains the same, with the deployment package containing both the demo.py script that uses the SDK and the option for exportable code.\n",
      "\n",
      "- The different architecture options (fast', accuracy', and balanced') will now be unified with the Models screen.\n",
      "\n",
      "Please note, the existing exportable code' option for running inference will still be included in the SDK deployment package for advanced users who wish to deploy the model without using the IntelÂ® Getiâ¢ SDK.\n",
      "-------------------- \n",
      "\n",
      "https://openvinotoolkit.github.io/geti-sdk/geti_sdk.deployment.data_models.html#module-geti_sdk.deployment.data_models 0.7887737554514889\n",
      "# geti_sdk.deployment.data_models package\n",
      "\n",
      "## Module contents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vector_search(db, \"What parameters does the DeployedModel class have?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f553ec94-83e9-4d48-aeb4-10f9ef329f94",
   "metadata": {},
   "source": [
    "## Building Document Functionality Using ChatGPT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4651daa0-8fc0-43b9-935b-ffcec51e0204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chunk', 'gpt-3.5-turbo', 'text-embedding-ada-002']\n"
     ]
    }
   ],
   "source": [
    "from superduperdb.ext.openai import OpenAIChatCompletion\n",
    "prompt = \"\"\"\n",
    "As an Intel GETI assistant, based on the provided documents and the question, answer the question.\n",
    "If the document does not provide an answer, offer a safe response without fabricating an answer.\n",
    "\n",
    "Documents:\n",
    "{context}\n",
    "\n",
    "Question: \"\"\"\n",
    "\n",
    "llm = OpenAIChatCompletion(identifier='gpt-3.5-turbo', prompt=prompt)\n",
    "\n",
    "db.add(llm)\n",
    "\n",
    "print(db.show('model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9010e120-c948-4ebd-8969-8d1614b8afb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa(db, query, vector_search_top_k=5):\n",
    "    logging.info(f\"QA query: {query}\")\n",
    "    collection = Collection(\"_outputs.elements.chunk\")\n",
    "    output, sources = db.predict(\n",
    "        model_name='gpt-3.5-turbo',\n",
    "        input=query,\n",
    "        context_select=collection.like(\n",
    "            Document({\"_outputs.elements.chunk\": query}),\n",
    "            vector_index=\"vector_index\",\n",
    "            n=vector_search_top_k,\n",
    "        ).find({}),\n",
    "        context_key=\"_outputs.elements.chunk.0.content\",\n",
    "    )\n",
    "    if sources:\n",
    "        sources = sorted(sources, key=lambda x: x.content[\"score\"], reverse=True)\n",
    "    return output, sources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d3389ba-0cea-4e77-ba93-79532ee8baa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-02-28 15:00:58] root INFO QA query: What parameters does the DeployedModel class have?\n",
      "[2024-02-28 15:00:58] httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "[2024-02-28 15:01:02] httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The DeployedModel class in the geti_sdk.deployment module has the following parameters:\n",
       "\n",
       "- Name\n",
       "- FPS throughput\n",
       "- Latency\n",
       "- Precision\n",
       "- Creation date\n",
       "- Size\n",
       "- Target device\n",
       "- Target device type\n",
       "- Previous revision id\n",
       "- Previous trained revision id\n",
       "- Score\n",
       "- Performance\n",
       "- ID\n",
       "- Label schema in sync\n",
       "- Model format\n",
       "- Has XAI head\n",
       "- Model status\n",
       "- Optimization methods\n",
       "- Optimization objectives\n",
       "- Optimization type\n",
       "- Version\n",
       "- Configurations\n",
       "- Hyper parameters\n",
       "\n",
       "These parameters are used to represent an Intel® Geti™ model that has been deployed for inference."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://openvinotoolkit.github.io/geti-sdk/geti_sdk.data_models.html#deployment-related-entities\n",
      "https://docs.geti.intel.com/on-prem/1.8/guide/deployments/deployments.html#deployments\n",
      "https://openvinotoolkit.github.io/geti-sdk/geti_sdk.deployment.html#geti_sdk.deployment.deployed_model.DeployedModel\n",
      "https://docs.geti.intel.com/on-prem/1.8/guide/release-notes/1.8/release-1.8.html#new-default-model-deployment\n",
      "https://openvinotoolkit.github.io/geti-sdk/geti_sdk.deployment.data_models.html#module-geti_sdk.deployment.data_models\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "output, sources = qa(db, \"What parameters does the DeployedModel class have?\")\n",
    "display(Markdown(output.content))\n",
    "for source in sources:\n",
    "    source_data = source.content['_source']\n",
    "    source_url = Collection('pages').find_one({\"_id\": source_data}).execute(db)['url']\n",
    "    data = source.outputs(\"elements\", 'chunk')\n",
    "    url = source_url + data['url']\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4e17c5e-f200-404e-9c4a-993d8990abad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# geti_sdk.deployment.data_models package\\uf0c1\\n\\n## Module contents\\uf0c1'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source.outputs(\"elements\", 'chunk')[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e866c224-cc9c-4283-8cee-a3c2c28a3b3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
